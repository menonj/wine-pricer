{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "boto3\n",
    "botocore\n",
    "tensorflow==1.15.2\n",
    "joblib\n",
    "numpy\n",
    "xgboost\n",
    "scikit-learn>=0.21.0\n",
    "seldon-core\n",
    "tornado>=6.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.24.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.12.42)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.15.42)\n",
      "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.15.2)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.18.3)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.1.1)\n",
      "Collecting scikit-learn>=0.21.0\n",
      "  Using cached scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Collecting seldon-core\n",
      "  Using cached seldon_core-1.5.1-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (6.0.4)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->-r requirements.txt (line 2)) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->-r requirements.txt (line 2)) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore->-r requirements.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore->-r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.15.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (3.11.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.28.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost->-r requirements.txt (line 7)) (1.2.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting Flask-cors<4.0.0\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: PyYAML<5.4 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (5.3.1)\n",
      "Requirement already satisfied: gunicorn<20.1.0,>=19.9.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (20.0.4)\n",
      "Requirement already satisfied: jsonschema<4.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (3.2.0)\n",
      "Processing /home/jovyan/.cache/pip/wheels/83/73/4c/0e331f57d4702becb1fca9d9148277aca96d127bd838faf85e/opentracing-2.4.0-py3-none-any.whl\n",
      "Processing /home/jovyan/.cache/pip/wheels/a8/37/30/d86c970966efbf6da89e8085db50f2be4cf36ca68513eff785/jaeger_client-4.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (2.22.0)\n",
      "Requirement already satisfied: prometheus-client<0.9.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (0.8.0)\n",
      "Collecting flatbuffers<2.0.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting redis<4.0.0\n",
      "  Using cached redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: Flask<2.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 9)) (46.1.3)\n",
      "Processing /home/jovyan/.cache/pip/wheels/ad/4b/2d/24ff0da0a0b53c7c77ce59b843bcceaf644c88703241e59615/Flask_OpenTracing-1.1.0-py3-none-any.whl\n",
      "Processing /home/jovyan/.cache/pip/wheels/9b/84/80/81f4dc4afff82cce892567df37472395abf1a1bb675caec1ec/grpcio_reflection-1.34.1-py3-none-any.whl\n",
      "Collecting grpcio-opentracing<1.2.0,>=1.1.4\n",
      "  Using cached grpcio_opentracing-1.1.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->-r requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema<4.0.0->seldon-core->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema<4.0.0->seldon-core->-r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema<4.0.0->seldon-core->-r requirements.txt (line 9)) (19.3.0)\n",
      "Processing /home/jovyan/.cache/pip/wheels/02/54/65/9f87de48fe8fcaaee30f279973d946ad55f9df56b93b3e78da/threadloop-1.0.2-py3-none-any.whl\n",
      "Processing /home/jovyan/.cache/pip/wheels/e0/38/fc/472fe18756b177b42096961f8bd3ff2dc5c5620ac399fce52d/thrift-0.13.0-cp36-cp36m-linux_x86_64.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->seldon-core->-r requirements.txt (line 9)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->seldon-core->-r requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->seldon-core->-r requirements.txt (line 9)) (2020.4.5.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 9)) (7.1.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 9)) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema<4.0.0->seldon-core->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask<2.0.0->seldon-core->-r requirements.txt (line 9)) (1.1.1)\n",
      "\u001b[31mERROR: grpcio-reflection 1.34.1 has requirement grpcio>=1.34.1, but you'll have grpcio 1.28.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn, Flask-cors, opentracing, threadloop, thrift, jaeger-client, flatbuffers, redis, Flask-OpenTracing, grpcio-reflection, grpcio-opentracing, seldon-core\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.20.3\n",
      "    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\n",
      "Successfully installed Flask-OpenTracing-1.1.0 Flask-cors-3.0.10 flatbuffers-1.12 grpcio-opentracing-1.1.4 grpcio-reflection-1.34.1 jaeger-client-4.3.0 joblib-1.0.0 opentracing-2.4.0 redis-3.5.3 scikit-learn-0.24.1 seldon-core-1.5.1 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import joblib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "\n",
    "logging.basicConfig(format='%(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(data_file_path):\n",
    "    # Get the data: original source is here: https://www.kaggle.com/zynicide/wine-reviews/data\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess(data):\n",
    "    # Do some preprocessing to limit the # of wine varities in the dataset\n",
    "    data = data[pd.notnull(data['country'])]\n",
    "    \n",
    "    print(\"preprocess 1\")\n",
    "    data = data[pd.notnull(data['price'])]\n",
    "    data = data.drop(data.columns[0], axis=1) \n",
    "\n",
    "    print(\"preprocess 2\")\n",
    "    variety_threshold = 500 # Anything that occurs less than this will be removed.\n",
    "    value_counts = data['variety'].value_counts()\n",
    "    to_remove = value_counts[value_counts <= variety_threshold].index\n",
    "    \n",
    "    print(\"preprocess 3\")\n",
    "    data.replace(to_remove, np.nan, inplace=True)\n",
    "    data = data[pd.notnull(data['variety'])]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_model(data):\n",
    "    train_size = int(len(data) * .8)\n",
    "    print (\"Train size: %d\" % train_size)\n",
    "    print (\"Test size: %d\" % (len(data) - train_size))\n",
    "    print(\"train model 1\")\n",
    "\n",
    "    # Train features\n",
    "    description_train = data['description'][:train_size]\n",
    "    print(\"train model 2\")\n",
    "    variety_train = data['variety'][:train_size]\n",
    "    labels_train = data['price'][:train_size]\n",
    "\n",
    "    # Create a tokenizer to preprocess our text descriptions\n",
    "    vocab_size = 12000 # This is a hyperparameter, experiment with different values for your dataset\n",
    "    print(\"train model 3\")\n",
    "    tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size, char_level=False)\n",
    "    tokenize.fit_on_texts(description_train) # only fit on train\n",
    "    print(\"train model 4\")\n",
    "\n",
    "    # Wide feature 1: sparse bag of words (bow) vocab_size vector \n",
    "    description_bow_train = tokenize.texts_to_matrix(description_train)\n",
    "    print(\"train model 5: wide feature 1 BOW\")\n",
    "\n",
    "    # Wide feature 2: one-hot vector of variety categories\n",
    "    # Use sklearn utility to convert label strings to numbered index\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(variety_train)\n",
    "    variety_train = encoder.transform(variety_train)\n",
    "    num_classes = np.max(variety_train) + 1\n",
    "    print(\"train model 6: wide feature 2\")\n",
    "\n",
    "    # Convert labels to one hot\n",
    "    variety_train = keras.utils.to_categorical(variety_train, num_classes)\n",
    "    print(\"train model 7: wide feature 2/one hot encode\")\n",
    "\n",
    "    # Define our wide model with the functional API\n",
    "    bow_inputs = layers.Input(shape=(vocab_size,))\n",
    "    variety_inputs = layers.Input(shape=(num_classes,))\n",
    "    merged_layer = layers.concatenate([bow_inputs, variety_inputs])\n",
    "    merged_layer = layers.Dense(256, activation='relu')(merged_layer)\n",
    "    predictions = layers.Dense(1)(merged_layer)\n",
    "    wide_model = keras.Model(inputs=[bow_inputs, variety_inputs], outputs=predictions)\n",
    "    print(\"train model 8\")\n",
    "\n",
    "    wide_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(wide_model.summary())\n",
    "    print(\"train model 9\")\n",
    "\n",
    "    # Deep model feature: word embeddings of wine descriptions\n",
    "    train_embed = tokenize.texts_to_sequences(description_train)\n",
    "    print(\"train model 10: deep model/train\")\n",
    "\n",
    "    max_seq_length = 170\n",
    "    train_embed = keras.preprocessing.sequence.pad_sequences(train_embed, maxlen=max_seq_length, padding=\"post\")\n",
    "\n",
    "    # Define our deep model with the Functional API\n",
    "    print(\"train model 11\")\n",
    "    deep_inputs = layers.Input(shape=(max_seq_length,))\n",
    "    embedding = layers.Embedding(vocab_size, 8, input_length=max_seq_length)(deep_inputs)\n",
    "\n",
    "    print(\"train model 12\")\n",
    "    embedding = layers.Flatten()(embedding)\n",
    "    embed_out = layers.Dense(1)(embedding)\n",
    "    deep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\n",
    "    #print(deep_model.summary())\n",
    "    print(\"train model 13\")\n",
    "\n",
    "    deep_model.compile(loss='mse',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    print(\"train model 14\")\n",
    "   \n",
    "    # Combine wide and deep into one model\n",
    "    merged_out = layers.concatenate([wide_model.output, deep_model.output])\n",
    "    merged_out = layers.Dense(1)(merged_out)\n",
    "    combined_model = keras.Model(wide_model.input + [deep_model.input], merged_out)\n",
    "    #print(combined_model.summary())\n",
    "\n",
    "    print(\"train model 15\")\n",
    "    combined_model.compile(loss='mse',\n",
    "                               optimizer='adam',\n",
    "                               metrics=['accuracy'])\n",
    "    print(\"train model 16\")\n",
    "\n",
    "    # Run training\n",
    "    combined_model.fit([description_bow_train, variety_train] + [train_embed], labels_train, epochs=10, batch_size=128)\n",
    "    return combined_model\n",
    "\n",
    "def eval_model(filename, data):\n",
    "    \"\"\"Evaluate the model performance.\"\"\"\n",
    "    print(\"eval model 1\")\n",
    "    \n",
    "    model = keras.models.load_model(filename)\n",
    "    \n",
    "    train_size = int(len(data) * .8)\n",
    "    description_test = data['description'][train_size:]\n",
    "    variety_test = data['variety'][train_size:]\n",
    "    \n",
    "    print(\"eval model 2\")\n",
    "    vocab_size = 12000 # This is a hyperparameter, experiment with different values for your dataset\n",
    "    tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size, char_level=False)\n",
    "    tokenize.fit_on_texts(description_test) # only fit on train\n",
    "    \n",
    "    print(\"eval model 3\")\n",
    "    # Test labels\n",
    "    labels_test = data['price'][train_size:]\n",
    "\n",
    "    print(\"eval model 4\")\n",
    "    # Wide feature 1: sparse bag of words (bow) vocab_size vector \n",
    "    description_bow_test = tokenize.texts_to_matrix(description_test)\n",
    "\n",
    "    print(\"eval model 5\")\n",
    "    # Wide feature 2: one-hot vector of variety categories\n",
    "    # Use sklearn utility to convert label strings to numbered index\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(variety_test)\n",
    "    variety_test = encoder.transform(variety_test)\n",
    "    num_classes = np.max(variety_test) + 1\n",
    "\n",
    "    print(\"eval model 6\")\n",
    "    # Convert labels to one hot\n",
    "    variety_test = keras.utils.to_categorical(variety_test, num_classes)\n",
    "\n",
    "    print(\"eval model 7\")\n",
    "    # Deep model feature: word embeddings of wine descriptions\n",
    "    test_embed = tokenize.texts_to_sequences(description_test)\n",
    "\n",
    "    print(\"eval model 8\")\n",
    "    max_seq_length = 170\n",
    "    test_embed = keras.preprocessing.sequence.pad_sequences(test_embed, maxlen=max_seq_length, padding=\"post\")\n",
    "\n",
    "    print(\"eval model 9\")\n",
    "    # Generate predictions\n",
    "    predictions = model.predict([description_bow_test, variety_test] + [test_embed])\n",
    "    \n",
    "    num_predictions = 40\n",
    "    diff = 0\n",
    "\n",
    "    for i in range(num_predictions):\n",
    "        val = predictions[i]\n",
    "        print(description_test.iloc[i])\n",
    "        print('Predicted: ', val[0], 'Actual: ', labels_test.iloc[i], '\\n')\n",
    "        diff += abs(val[0] - labels_test.iloc[i])\n",
    "        \n",
    "    # Compare the average difference between actual price and the model's predicted price\n",
    "    print('Average prediction difference: ', diff / num_predictions)\n",
    "    print(\"eval model 11\")\n",
    "\n",
    "def save_model(model, model_file):\n",
    "    model.save(model_file)\n",
    "    print(\"Model export success: %s\", model_file)\n",
    "    \n",
    "class WinePricer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_input = \"wine_data.csv\"\n",
    "        self.n_estimators = 50\n",
    "        self.learning_rate = 0.1\n",
    "        self.model_file = \"trained_wine_model.dat\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self):\n",
    "        data = read_input(self.train_input)\n",
    "        data = preprocess(data)\n",
    "        model = train_model(data)\n",
    "        save_model(model, self.model_file)\n",
    "        eval_model(self.model_file, data)\n",
    "\n",
    "    def predict(self, X, feature_names=None):\n",
    "        \"\"\"Predict using the model for given ndarray.\"\"\"\n",
    "        if not self.model:\n",
    "            self.model = keras.models.load_model(self.model_file)\n",
    "        # Do any preprocessing\n",
    "        prediction = self.model.predict(data=X)\n",
    "        # Do any postprocessing\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess 1\n",
      "preprocess 2\n",
      "preprocess 3\n",
      "Train size: 95646\n",
      "Test size: 23912\n",
      "train model 1\n",
      "train model 2\n",
      "train model 3\n",
      "train model 4\n",
      "train model 5: wide feature 1 BOW\n",
      "train model 6: wide feature 2\n",
      "train model 7: wide feature 2/one hot encode\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model 8\n",
      "train model 9\n",
      "train model 10: deep model/train\n",
      "train model 11\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model 12\n",
      "train model 13\n",
      "train model 14\n",
      "train model 15\n",
      "train model 16\n",
      "Train on 95646 samples\n",
      "Epoch 1/10\n",
      "95646/95646 [==============================] - 28s 289us/sample - loss: 1326.6898 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "95646/95646 [==============================] - 25s 259us/sample - loss: 1015.7144 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "95646/95646 [==============================] - 25s 262us/sample - loss: 894.1288 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "95646/95646 [==============================] - 25s 262us/sample - loss: 783.0724 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "95646/95646 [==============================] - 25s 260us/sample - loss: 675.0372 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "95646/95646 [==============================] - 25s 260us/sample - loss: 570.5601 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "95646/95646 [==============================] - 25s 261us/sample - loss: 470.8375 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "95646/95646 [==============================] - 25s 262us/sample - loss: 385.6529 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "95646/95646 [==============================] - 25s 264us/sample - loss: 310.2801 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "95646/95646 [==============================] - 25s 265us/sample - loss: 249.7170 - acc: 0.0000e+00\n",
      "Model export success: %s trained_wine_model.dat\n",
      "eval model 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval model 2\n",
      "eval model 3\n",
      "eval model 4\n",
      "eval model 5\n",
      "eval model 6\n",
      "eval model 7\n",
      "eval model 8\n",
      "eval model 9\n",
      "A heavy wine, atypical of the appellation, which usually produces light-bodied and elegant Pinots. There's something earthy and thick about this one, almost rustic. The flavors are fruity but it lacks the delicacy an Anderson Valley Pinot Noir ought to have. And there's an unpleasant sting of acidity.\n",
      "Predicted:  38.960835 Actual:  16.0 \n",
      "\n",
      "Seems like the hot sun of central Spain got to this three-grape blend because it's undeniably sweet and baked. The aromas are pure raisin, while the palate is drawn down and syrupy. Too thick and sweet for its own good.\n",
      "Predicted:  25.963655 Actual:  18.0 \n",
      "\n",
      "Snappy, crisp and green on the nose, but well balanced and showing the good side of Chilean SB, which is oceanic, fresh, green and citrusy. Flavors of lime, grapefruit and passion fruit are true, while the finish is fruity at first before breaking up.\n",
      "Predicted:  13.438766 Actual:  12.0 \n",
      "\n",
      "A pleasantly dry, fruity Syrah, with spicy flavors of berries, currants, cola and plums, and a tangy streak of grilled bacon. Drink this soft wine now.\n",
      "Predicted:  21.429256 Actual:  18.0 \n",
      "\n",
      "Floral aromas with a touch of citrus lead this assertive Chardonnay. The flavors—vanilla, fresh fruit, spice—are wrapped in a wave of smooth oak. An elegant wine good for pairing with salad and poultry dishes.\n",
      "Predicted:  28.88439 Actual:  9.0 \n",
      "\n",
      "Smooth apple-pear and buttery, toasty aromas open to like flavors. That fruit core is offset by mild mango accents and lots of spicy oak, which continue through the long finish. For fans of rich, oaky full-bodied Chardonnay.\n",
      "Predicted:  33.098522 Actual:  30.0 \n",
      "\n",
      "This features plum, berry and leather aromas that are accented by a hint of lemon. It feels narrow and drawing, with black fruit, herb and carob flavors. The finish, which doesn't taste green or herbal, is better and wider.\n",
      "Predicted:  34.14012 Actual:  14.0 \n",
      "\n",
      "Smoky almost to the point of being a one-noter, but as it warms the heavy toast gives way to apple, orange and coconut flavors. The savior is prime acidity, which creates a raciness that rivals the woodiness. Imported by RM Imports.\n",
      "Predicted:  19.564259 Actual:  13.0 \n",
      "\n",
      "A lacy spray of sweetness extends from start to finish on this fruity, fresh Kabinett. Fresh apple, pear and tangerine are honeyed and lip smacking, energized by a streak of lemonade acidity on the finish.\n",
      "Predicted:  13.352241 Actual:  20.0 \n",
      "\n",
      "One of a pair of wines under the Grilos name, this Dão is solid and tannic, still young, with a firm core of dryness surrounded by a more fruity black cherry component. It needs aging for a year.\n",
      "Predicted:  24.351784 Actual:  11.0 \n",
      "\n",
      "Pungent aromas of ammonia and cat pee feature hints of gooseberry and grapefruit. Once past the nose, things settle down, with melon, fig and grapefruit flavors that finish a bit rough.\n",
      "Predicted:  11.962222 Actual:  13.0 \n",
      "\n",
      "Almost equal parts Syrah, Grenache and Mourvèdre from the estate vineyard. This is a real step down from Bone Rock, mainly because of overripeness. It shows sugary sweet cherry pie, vanilla and caramel flavors, and the alcohol weighs in at 16%. What a notoriously hard vintage this was.\n",
      "Predicted:  22.43612 Actual:  45.0 \n",
      "\n",
      "Despite originating from Hawkes Bay on New Zealand's North Island, this could pass for Marlborough. It's not overly big or ripe, but well-rounded without losing any sense of varietal character. Hints of tomato leaf and white grapefruit linger elegantly on the finish.\n",
      "Predicted:  27.498133 Actual:  15.0 \n",
      "\n",
      "Generally speaking, this is a respectable Priorat from a tough year. The nose is berry packed, while the flavors run toward toast-covered raspberry. It's typically grabby and expansive, as any good young wine from the area will be, yet you can drink it now with meat.\n",
      "Predicted:  23.5661 Actual:  68.0 \n",
      "\n",
      "This lively sparkler opens with ripe aromas of apricot, honey and candied fruit that are backed by notes of freshly baked bread and toasted almond. Made from Chardonnay that rests on the lees for three years, it also shows creamy richness on the finish.\n",
      "Predicted:  41.27959 Actual:  39.0 \n",
      "\n",
      "A beautiful wine, smooth, polished, with rounded corners. Its intense fruit is balanced and harmonizes well with the sweet tannins, the layers of wood and the red berry flavors. Certainly powerful, it impresses equally with the pleasure it will give in 7–10 years.\n",
      "Predicted:  16.4864 Actual:  319.0 \n",
      "\n",
      "A great Weir from Williams Selyem, showing a complex balance on the warm-cool-climate spectrum. It's very ripe in blackberries and cherries, but crisp in acidity, firm in tannins, and totally dry. Balanced and delicious but young, this wine will benefit from six years in a proper cellar.\n",
      "Predicted:  38.321606 Actual:  56.0 \n",
      "\n",
      "This is a light red that can be consumed slightly chilled. Aromas include strawberries, raspberries and garden herbs, with flavors that are briary and herbal, yet rounded, layered with plenty of fresh fruit. 60% Nero d'Avola, 40% Frappato.\n",
      "Predicted:  11.838975 Actual:  23.0 \n",
      "\n",
      "Edging towards mineral, this is a perfumed Riesling, its white fruits bright and delicate, fitting well into the crisp acidity. It has a textured bite to it, bringing out the stony character. Worth aging 2–3 years.\n",
      "Predicted:  29.67068 Actual:  15.0 \n",
      "\n",
      "There's a nutty, smoky quality to this Cartizze Prosecco that adds depth and dimension beyond the wine's more characteristic floral and fresh fruit notes. This is a well-balanced wine thanks to a playful harmony between delicate sweetness and spicy crispness on the finish.\n",
      "Predicted:  35.21263 Actual:  36.0 \n",
      "\n",
      "This 2007 Brunello is a big, opulent pleasure bomb with lavish layers, loaded thickly on top of each other: Chocolate fudge, dark cherry, blackberry preserves, rum cake, prune, exotic spice, pipe tobacco, cola, humus and leather. It shows huge personality, intensity and staying power, too. All that density is backed by solid tannins and a steady firmness. Hold 10–15 years.\n",
      "Predicted:  48.711365 Actual:  50.0 \n",
      "\n",
      "Although this Cabernet is ripe, it's harsh in the mouth, with a tart scour. Drink it with rich fare, like burgers.\n",
      "Predicted:  31.624699 Actual:  26.0 \n",
      "\n",
      "Sharp and candied, with Lifesaver sweet-and-sour cherries and cola softened with smoky wood notes. The tannins are sandpapery and firm. Drink now with rich beef and lamb.\n",
      "Predicted:  41.395153 Actual:  39.0 \n",
      "\n",
      "Here's a Cab that makes a play for supremecy in a very crowded field. The wine, which contains a drop of Merlot, is solidly in the modern, international, Napa cult style. Soft and appealing for its blackberry, currant, plum and vanilla oak flavors, it's drinkable now, but such are the tannins that it will be better after 2008.\n",
      "Predicted:  60.599228 Actual:  125.0 \n",
      "\n",
      "Nice flavors of baked red cherries, cola, bacon and spice, and the finish is dry and clean. Lacks the acidity to make all this vibrant, though.\n",
      "Predicted:  31.675606 Actual:  20.0 \n",
      "\n",
      "Ripe and seductive with cherries and sweet cranberries, this young, tight, stylish Pinot has nothing but clear sailing ahead. There's excellent concentration through the midpalate, a light touch with the new oak, and beautifully textured fruit throughout. Love it now or leave it for later; you can't lose either way.\n",
      "Predicted:  62.053974 Actual:  38.0 \n",
      "\n",
      "There's a good wine in here, but it's buried under so much oak, it's hard to taste it. The flavors of caramel and butterscotch swamp the underlying pineapples. Oakophiles will love it.\n",
      "Predicted:  24.25617 Actual:  20.0 \n",
      "\n",
      "This South Australia Viognier comes across as restrained on the nose, offering up little more than some vaguely smoky or flinty notes, then delivers melon, pear and spice notes via a full-bodied, slightly oily-textured palate. Turns crisp and citrusy on the finish.\n",
      "Predicted:  47.49716 Actual:  14.0 \n",
      "\n",
      "Although this Cab is nearly seven years old, it's still hard and tough in tannins, and the tart acidity suggests it will remain unbalanced for the rest of its life. If you can get past the sharpness, it has dry flavors of blackberries, cherries and currants.\n",
      "Predicted:  68.75256 Actual:  150.0 \n",
      "\n",
      "There's a pretty mineral context here that frames a juicy core of honey, almond and peach. This oak-aged Chardonnay is simple but genuine and would make a good companion to endive salad with blood oranges and black olives.\n",
      "Predicted:  30.239439 Actual:  12.0 \n",
      "\n",
      "A crisp medium-bodied Cab with good varietal definition, Strathewen Hill's 2005 Cabernet Sauvignon boasts slightly minty cassis-inflected fruit, vanilla and chocolate notes and a smooth mouthfeel. Supple tannins on the finish suggest this will age well through 2012 or so.\n",
      "Predicted:  36.092472 Actual:  30.0 \n",
      "\n",
      "Santenots, like most premier crus, is located halfway down the slope. This is structured, yet it remains ripe and fruity, and it shows off the voluptuous texture that's characteristic of Volnay. There is fine balance between the acidity and fruit, and the dry core promises aging potential.\n",
      "Predicted:  32.09963 Actual:  70.0 \n",
      "\n",
      "So forward and appealing right now, it's hard to resist, but this could gain a little traction with a two or three years of age. It's polished in cherry jam and blackberry-pie filling, with earthy tones of coffee, figs, sage and cedar. The intricate tannins play nicely against the acidity, which gives it an upscale feeling.\n",
      "Predicted:  15.974417 Actual:  24.0 \n",
      "\n",
      "Not clear if this is a long-term ager, but you definitely want to give it a good decanting of at least four hours. You should even pour the glasses and let them to stand for an hour. The interaction with oxygen will open the wine up and allow it to express its herbal, soft deliciousness. Airing also emphasizes the cherry and red currant fruit, letting the palate enjoy this complex, tannic wine at its best.\n",
      "Predicted:  60.96416 Actual:  70.0 \n",
      "\n",
      "This crisp, acidic Sauvignon Blanc leans green with pea and canned bean aromas, but it offers enough green apple and citrus character for enjoyment. The palate is zesty and lemony, with flavors that include green veggies, herbs, limes and tangerines. The finish is mildly green and citrusy.\n",
      "Predicted:  29.451323 Actual:  10.0 \n",
      "\n",
      "Quite youthful, this is a sprightly and charming wine, evincing aromas of white flowers and peaches. The crisp palate has lemon-lime notes and a chalky edge. Medium, citrusy finish. Drink now or hold a few years for more complexity.\n",
      "Predicted:  37.16755 Actual:  33.0 \n",
      "\n",
      "Not very expressive, it opens with subdued aromas that recall lemon peel and toast. These carry over to the palate along with a note of butterscotch. There's not enough fruit richness to support the racy acidity.\n",
      "Predicted:  27.182407 Actual:  19.0 \n",
      "\n",
      "This blend of Cabernet, Syrah and Zinfandel is simple, soft and overtly sweet. It has berry, cherry, currant and chocolate flavors that taste sugary, like pie filling.\n",
      "Predicted:  20.709244 Actual:  25.0 \n",
      "\n",
      "Almost colorless wine, with a limpid texture and soft berry fruits, but with layers of acidity over the softness. Ultimately, it is a wine that needs to be enjoyed with food, and can be opened well in advance.\n",
      "Predicted:  20.163233 Actual:  29.0 \n",
      "\n",
      "From the famed Monte Rosso Vineyard Ed Sbragia has crafted a wine that needs time in the cellar. Strong, hard tannins shut down the currant and cherry fruit, making the wine astringent. But it's a very fine mountain Cab, and will reward aging. Should begin to be approachable by 2010, and continue developing for many more years.\n",
      "Predicted:  15.893215 Actual:  50.0 \n",
      "\n",
      "Average prediction difference:  23.046814918518066\n",
      "eval model 11\n"
     ]
    }
   ],
   "source": [
    "model = WinePricer()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: leb547-kubeflow-pipeline-data\n"
     ]
    }
   ],
   "source": [
    "import random, string\n",
    "HASH = ''.join([random.choice(string.ascii_lowercase) for n in range(3)] + [random.choice(string.digits) for n in range(3)])\n",
    "AWS_REGION = 'us-east-1'\n",
    "!aws s3 mb s3://{HASH}'-kubeflow-pipeline-data' --region $AWS_REGION --endpoint-url https://s3.us-east-1.amazonaws.com\n",
    "#!aws s3 mb s3://{HASH}'-kubeflow-pipeline-data' --region $AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow import fairing\n",
    "from kubeflow.fairing import TrainJob\n",
    "from kubeflow.fairing.backends import KubeflowAWSBackend\n",
    "\n",
    "\n",
    "from kubeflow import fairing\n",
    "\n",
    "FAIRING_BACKEND = 'KubeflowAWSBackend'\n",
    "\n",
    "AWS_ACCOUNT_ID = fairing.cloud.aws.guess_account_id()\n",
    "AWS_REGION = 'us-east-1'\n",
    "DOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com'.format(AWS_ACCOUNT_ID, AWS_REGION)\n",
    "S3_BUCKET = f'{HASH}-kubeflow-pipeline-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if FAIRING_BACKEND == 'KubeflowAWSBackend':\n",
    "    from kubeflow.fairing.builders.cluster.s3_context import S3ContextSource\n",
    "    BuildContext = S3ContextSource(\n",
    "        aws_account=AWS_ACCOUNT_ID, region=AWS_REGION,\n",
    "        bucket_name=S3_BUCKET\n",
    "    )\n",
    "\n",
    "BackendClass = getattr(importlib.import_module('kubeflow.fairing.backends'), FAIRING_BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default base docker image: registry.hub.docker.com/library/python:3.6.9\n",
      "Using builder: <class 'kubeflow.fairing.builders.cluster.cluster.ClusterBuilder'>\n",
      "Building the docker image.\n",
      "Building image using cluster builder.\n",
      "/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Creating docker context: /tmp/fairing_context_q3r5iyrf\n",
      "/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Not able to find aws credentials secret: aws-secret\n",
      "Waiting for fairing-builder-5t22s-n66kn to start...\n",
      "Waiting for fairing-builder-5t22s-n66kn to start...\n",
      "Waiting for fairing-builder-5t22s-n66kn to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0006] Resolved base name registry.hub.docker.com/library/python:3.6.9 to registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Resolved base name registry.hub.docker.com/library/python:3.6.9 to registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Error while retrieving image from cache: getting file info: stat /cache/sha256:036d4ab50fa49df89e746cf1b5369c88db46e8af2fbd08531788e7d920e9a491: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Built cross stage deps: map[]\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Error while retrieving image from cache: getting file info: stat /cache/sha256:036d4ab50fa49df89e746cf1b5369c88db46e8af2fbd08531788e7d920e9a491: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image registry.hub.docker.com/library/python:3.6.9\n",
      "\u001b[36mINFO\u001b[0m[0006] Unpacking rootfs as cmd COPY /app//requirements.txt /app/ requires it.\n",
      "\u001b[36mINFO\u001b[0m[0021] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0027] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0027] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0027] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0027] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0027] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0027] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0027] Using files from context: [/kaniko/buildcontext/app/requirements.txt]\n",
      "\u001b[36mINFO\u001b[0m[0027] COPY /app//requirements.txt /app/\n",
      "\u001b[36mINFO\u001b[0m[0027] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0027] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0027] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0027] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/db/a7e290eb77632c9d25247977bbfc99aef9cd59f7c13eea69f8fea44404af/boto3-1.16.63-py2.py3-none-any.whl (130kB)\n",
      "Collecting botocore\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/05/0a955f0c92bec7da076fbbc73926dfb13fab8e2b88de7f8eb17c443f28f0/botocore-1.19.63-py2.py3-none-any.whl (7.2MB)\n",
      "Collecting tensorflow==1.15.2\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
      "Collecting joblib\n",
      "  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "Collecting numpy\n",
      "  Downloading https://files.pythonhosted.org/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8MB)\n",
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/57/bf5026701c384decd2b995eb39d86587a103ba4eb26f8a9b1811db0896d3/xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5MB)\n",
      "Collecting scikit-learn>=0.21.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/4c/6111b9a325f29527d7f262e2ee8c730d354b47a728d955e186dacad57a0d/scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n",
      "Collecting seldon-core\n",
      "  Downloading https://files.pythonhosted.org/packages/36/53/f9661ab010ac4cd7799ddde6351ca61d545c07d06445144ba9aeb152496d/seldon_core-1.5.1-py3-none-any.whl (127kB)\n",
      "Collecting tornado>=6.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/85/26/e710295dcb4aac62b08f22d07efc899574476db37532159a7f71713cdaf2/tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version != \"3.4\"\n",
      "  Downloading https://files.pythonhosted.org/packages/23/fc/8a49991f7905261f9ca9df5aa9b58363c3c821ce3e7f671895442b7100f2/urllib3-1.26.3-py2.py3-none-any.whl (137kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/fd/247ef25f5ec5f9acecfbc98ca3c6aaf66716cf52509aca9a93583d410493/protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting six>=1.10.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl (127kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading https://files.pythonhosted.org/packages/88/55/357022b111b856cadaf63d718d79861fc6215b848eff38b2fbfb9d5c47bd/grpcio-1.35.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from tensorflow==1.15.2->-r requirements.txt (line 4)) (0.33.6)\n",
      "Collecting scipy\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting grpcio-reflection<1.35.0\n",
      "  Downloading https://files.pythonhosted.org/packages/16/4c/7b05077d9a3d7e60df742ab507578c501cc05861f4328ba4919fc799d72b/grpcio-reflection-1.34.1.tar.gz\n",
      "Collecting grpcio-opentracing<1.2.0,>=1.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/db/82/2fcad380697c3dab25de76ee590bcab3eb9bbfb4add916044d7e83ec2b10/grpcio_opentracing-1.1.4-py3-none-any.whl\n",
      "Collecting Flask<2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB)\n",
      "Collecting flatbuffers<2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting opentracing<2.5.0,>=2.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/51/28/2dba4e3efb64cc59d4311081a5ddad1dde20a19b69cd0f677cdb2f2c29a6/opentracing-2.4.0.tar.gz (46kB)\n",
      "Collecting requests<3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 9)) (41.6.0)\n",
      "Collecting Flask-OpenTracing<1.2.0,>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/58/6c/6417701ba5ecc8854670c6db3207bcc3e5fbc96289a7cb18d5516d99a1c6/Flask-OpenTracing-1.1.0.tar.gz\n",
      "Collecting PyYAML<5.4\n",
      "  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "Collecting redis<4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "Collecting jaeger-client<4.4.0,>=4.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/75/17a937a61135671cebc175ab5c299dc0f7477042469482fd9a6f91262c68/jaeger-client-4.3.0.tar.gz (81kB)\n",
      "Collecting prometheus-client<0.9.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/0e/554a265ffdc56e1494ef08e18f765b0cdec78797f510c58c45cf37abb4f4/prometheus_client-0.8.0-py2.py3-none-any.whl (53kB)\n",
      "Collecting Flask-cors<4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
      "Collecting gunicorn<20.1.0,>=19.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
      "Collecting jsonschema<4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl (96kB)\n",
      "Collecting h5py\n",
      "  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/c2/1eece8c95ddbc9b1aeb64f5783a9e07a286de42191b7204d67b7496ddf35/Jinja2-2.11.3-py2.py3-none-any.whl (125kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl (147kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "Collecting threadloop<2,>=1\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/1d/8398c1645b97dc008d3c658e04beda01ede3d90943d40c8d56863cf891bd/threadloop-1.0.2.tar.gz\n",
      "Collecting thrift\n",
      "  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/ed/da40116a204abb5c4dd1d929346d33e0d29cedb2cedd18ea98f0385dcd92/importlib_metadata-3.4.0-py3-none-any.whl\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/70/fd441df751ba8b620e03fd2d2d9ca902103119616f0f6cc42e6405035062/pyrsistent-0.17.3.tar.gz (106kB)\n",
      "Collecting cached-property; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/d3/75cddfad6ca1d1bb3a017cece499a65e54ceb4583800f1256b8ad07bb57f/MarkupSafe-1.1.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "Building wheels for collected packages: termcolor, wrapt, gast, grpcio-reflection, opentracing, Flask-OpenTracing, PyYAML, jaeger-client, threadloop, thrift, pyrsistent\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=c7681657af728050587290d4f10911e47b5bc67af8a26a2e188fcf3941fc8b90\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=75225 sha256=fb6359140145c9f39a0d7ab9a708590d9f52fcb0bc8062360e231a9d4f12a343\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=c1fa8c89059fe1ee52166aea3b5beb1ac863783313e9fbf6ed42a67809b7c6ab\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for grpcio-reflection (setup.py): started\n",
      "  Building wheel for grpcio-reflection (setup.py): finished with status 'done'\n",
      "  Created wheel for grpcio-reflection: filename=grpcio_reflection-1.34.1-cp36-none-any.whl size=14413 sha256=23aa521ba5b431068f7bf3f4aa07c8b6a38164a1bb9ccaef7b7fc66cb422dcd9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/85/0e/79/919373e994613ef41ec9ffcd6ee9dd7952ab0dc2bbf963d209\n",
      "  Building wheel for opentracing (setup.py): started\n",
      "  Building wheel for opentracing (setup.py): finished with status 'done'\n",
      "  Created wheel for opentracing: filename=opentracing-2.4.0-cp36-none-any.whl size=51401 sha256=fcad06b2ca85eaab8eb5133bf676476504d7bc6eb9c1e3ddd0940db5a38e5690\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/27/b6/a0/d0309988a0dd5623c34469b151e4d7b0e6271b28a8bcccb440\n",
      "  Building wheel for Flask-OpenTracing (setup.py): started\n",
      "  Building wheel for Flask-OpenTracing (setup.py): finished with status 'done'\n",
      "  Created wheel for Flask-OpenTracing: filename=Flask_OpenTracing-1.1.0-cp36-none-any.whl size=9071 sha256=07bb11b1ad3c642d02da35895725fafa2548f60c0d4728c9936e16635542802e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/7b/dc/25/3cf0b35c129232ee596c413f13d1d1f5a8e38c427266276dfd\n",
      "  Building wheel for PyYAML (setup.py): started\n",
      "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=480454 sha256=f3ff680c76f0d755026ffe9a8dc1ab7159d23ae902cd1196a90ed00b4cbea36e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for jaeger-client (setup.py): started\n",
      "  Building wheel for jaeger-client (setup.py): finished with status 'done'\n",
      "  Created wheel for jaeger-client: filename=jaeger_client-4.3.0-cp36-none-any.whl size=64290 sha256=bd0010d16d5e6128f913f79271da12424148ef92a42ede99497b02e88096b22b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/a4/46/e1/316c9ea87f46b9685c2f4258950a27bad1ae072c56224e4489\n",
      "  Building wheel for threadloop (setup.py): started\n",
      "  Building wheel for threadloop (setup.py): finished with status 'done'\n",
      "  Created wheel for threadloop: filename=threadloop-1.0.2-cp36-none-any.whl size=3425 sha256=80f96155929708b683bd8b46bbf4176527381d15cf2614b814a18114e811889f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/d7/7a/30/d212623a4cd34f6cce400f8122b1b7af740d3440c68023d51f\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.13.0-cp36-cp36m-linux_x86_64.whl size=483930 sha256=229b36c79592ad8a4d4207c2ebfdb5128fd0f36e132212a66f94c94702c29d32\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp36-cp36m-linux_x86_64.whl size=122519 sha256=62a4d11dc5259e5cb2cc784c8d4afbb9c0bc2385e718b9a9ce7f6db01ecb9a1f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8l4thko/wheels/f0/b8/de/b593ad311be4eb458499d100db081e453576032272398b7ddc\n",
      "Successfully built termcolor wrapt gast grpcio-reflection opentracing Flask-OpenTracing PyYAML jaeger-client threadloop thrift pyrsistent\n",
      "Installing collected packages: pytz, numpy, six, python-dateutil, pandas, jmespath, urllib3, botocore, s3transfer, boto3, opt-einsum, termcolor, protobuf, wrapt, gast, werkzeug, grpcio, absl-py, typing-extensions, zipp, importlib-metadata, markdown, tensorboard, astor, cached-property, h5py, keras-applications, google-pasta, tensorflow-estimator, keras-preprocessing, tensorflow, joblib, scipy, xgboost, threadpoolctl, scikit-learn, grpcio-reflection, opentracing, grpcio-opentracing, MarkupSafe, Jinja2, click, itsdangerous, Flask, flatbuffers, certifi, chardet, idna, requests, Flask-OpenTracing, PyYAML, redis, tornado, threadloop, thrift, jaeger-client, prometheus-client, Flask-cors, gunicorn, attrs, pyrsistent, jsonschema, seldon-core\n",
      "Successfully installed Flask-1.1.2 Flask-OpenTracing-1.1.0 Flask-cors-3.0.10 Jinja2-2.11.3 MarkupSafe-1.1.1 PyYAML-5.3.1 absl-py-0.11.0 astor-0.8.1 attrs-20.3.0 boto3-1.16.63 botocore-1.19.63 cached-property-1.5.2 certifi-2020.12.5 chardet-4.0.0 click-7.1.2 flatbuffers-1.12 gast-0.2.2 google-pasta-0.2.0 grpcio-1.35.0 grpcio-opentracing-1.1.4 grpcio-reflection-1.34.1 gunicorn-20.0.4 h5py-3.1.0 idna-2.10 importlib-metadata-3.4.0 itsdangerous-1.1.0 jaeger-client-4.3.0 jmespath-0.10.0 joblib-1.0.0 jsonschema-3.2.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 opentracing-2.4.0 opt-einsum-3.3.0 pandas-1.1.5 prometheus-client-0.8.0 protobuf-3.14.0 pyrsistent-0.17.3 python-dateutil-2.8.1 pytz-2021.1 redis-3.5.3 requests-2.25.1 s3transfer-0.3.4 scikit-learn-0.24.1 scipy-1.5.4 seldon-core-1.5.1 six-1.15.0 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 threadloop-1.0.2 threadpoolctl-2.1.0 thrift-0.13.0 tornado-6.1 typing-extensions-3.7.4.3 urllib3-1.26.\n",
      "3 werkzeug-1.0.1 wrapt-1.12.1 xgboost-1.3.3 zipp-3.4.0\n",
      "WARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0088] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0125] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0126] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0126] Taking snapshot of files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not able to find aws credentials secret: aws-secret\n",
      "The job fairing-job-9bs5k launched.\n",
      "Waiting for fairing-job-9bs5k-zm9hr to start...\n",
      "Waiting for fairing-job-9bs5k-zm9hr to start...\n",
      "Waiting for fairing-job-9bs5k-zm9hr to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2021-02-02 19:34:09.290172: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2021-02-02 19:34:09.298040: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499995000 Hz\n",
      "2021-02-02 19:34:09.304029: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cac6bf9cf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-02-02 19:34:09.304079: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "preprocess 1\n",
      "preprocess 2\n",
      "preprocess 3\n",
      "Train size: 95646\n",
      "Test size: 23912\n",
      "train model 1\n",
      "train model 2\n",
      "train model 3\n",
      "train model 4\n",
      "train model 5: wide feature 1 BOW\n",
      "train model 6: wide feature 2\n",
      "train model 7: wide feature 2/one hot encode\n",
      "train model 8\n",
      "train model 9\n",
      "train model 10: deep model/train\n",
      "train model 11\n",
      "train model 12\n",
      "train model 13\n",
      "train model 14\n",
      "train model 15\n",
      "train model 16\n",
      "Train on 95646 samples\n",
      "Epoch 1/10\n",
      " 1280/95646 [..............................] - ETA: 1:06 - loss: 2753.9105 - acc: 0.0000e+\n",
      " 2688/95646 [..............................] - ETA: 48s - loss: 2241.8329 - acc: 0.0000e+00\n",
      " 3584/95646 [>.............................] - ETA: 43s - loss: 1957.9840 - acc: 0.0000e+0\n",
      " 4992/95646 [>.............................] - ETA: 39s - loss: 1885.4501 - acc: 0.0000e+0\n",
      " 6272/95646 [>.............................] - ETA: 36s - loss: 1625.2110 - acc: 0.0000e+0\n",
      " 7680/95646 [=>............................] - ETA: 34s - loss: 1549.3847 - acc: 0.0000e+0\n",
      " 9216/95646 [=>............................] - ETA: 32s - loss: 1512.3848 - acc: 0.0000e+0\n",
      "10496/95646 [==>...........................] - ETA: 31s - loss: 1588.4302 - acc: 0.0000e+0\n",
      "11648/95646 [==>...........................] - ETA: 30s - loss: 1580.2394 - acc: 0.0000e+0\n",
      "12928/95646 [===>..........................] - ETA: 29s - loss: 1487.1294 - acc: 0.0000e+0\n",
      "14464/95646 [===>..........................] - ETA: 28s - loss: 1436.8639 - acc: 0.0000e+0\n",
      "15872/95646 [===>..........................] - ETA: 27s - loss: 1407.3292 - acc: 0.0000e+0\n",
      "16768/95646 [====>.........................] - ETA: 27s - loss: 1355.1207 - acc: 0.0000e+0\n",
      "18304/95646 [====>.........................] - ETA: 27s - loss: 1312.5440 - acc: 0.0000e+0\n",
      "19584/95646 [=====>........................] - ETA: 26s - loss: 1261.9886 - acc: 0.0000e+0\n",
      "20864/95646 [=====>........................] - ETA: 25s - loss: 1240.6760 - acc: 0.0000e+0\n",
      "22016/95646 [=====>........................] - ETA: 25s - loss: 1224.0344 - acc: 0.0000e+0\n",
      "23168/95646 [======>.......................] - ETA: 25s - loss: 1282.8480 - acc: 0.0000e+0\n",
      "24576/95646 [======>.......................] - ETA: 24s - loss: 1269.9670 - acc: 0.0000e+0\n",
      "25856/95646 [=======>......................] - ETA: 24s - loss: 1259.7530 - acc: 0.0000e+0\n",
      "27264/95646 [=======>......................] - ETA: 23s - loss: 1244.5494 - acc: 0.0000e+0\n",
      "28460/95646 [=======>......................] - ETA: 23s - loss: 1228.2623 - acc: 0.0000e+0\n",
      "29440/95646 [========>.....................] - ETA: 23s - loss: 1258.4956 - acc: 0.0000e+0\n",
      "30848/95646 [========>.....................] - ETA: 22s - loss: 1268.0309 - acc: 0.0000e+0\n",
      "32000/95646 [=========>....................] - ETA: 21s - loss: 1253.4640 - acc: 0.0000e+0\n",
      "33408/95646 [=========>....................] - ETA: 21s - loss: 1251.9833 - acc: 0.0000e+0\n",
      "34688/95646 [=========>....................] - ETA: 20s - loss: 1228.8227 - acc: 0.0000e+0\n",
      "35968/95646 [==========>...................] - ETA: 20s - loss: 1252.5580 - acc: 0.0000e+0\n",
      "37504/95646 [==========>...................] - ETA: 20s - loss: 1244.2617 - acc: 0.0000e+0\n",
      "38784/95646 [===========>..................] - ETA: 19s - loss: 1230.7604 - acc: 0.0000e+0\n",
      "40064/95646 [===========>..................] - ETA: 18s - loss: 1220.5534 - acc: 0.0000e+0\n",
      "41216/95646 [===========>..................] - ETA: 18s - loss: 1202.1594 - acc: 0.0000e+0\n",
      "42752/95646 [============>.................] - ETA: 17s - loss: 1302.1963 - acc: 0.0000e+0\n",
      "44288/95646 [============>.................] - ETA: 17s - loss: 1281.3156 - acc: 0.0000e+0\n",
      "45568/95646 [=============>................] - ETA: 16s - loss: 1271.8032 - acc: 0.0000e+0\n",
      "46976/95646 [=============>................] - ETA: 16s - loss: 1263.0340 - acc: 0.0000e+0\n",
      "48128/95646 [==============>...............] - ETA: 16s - loss: 1245.3760 - acc: 0.0000e+0\n",
      "49664/95646 [==============>...............] - ETA: 15s - loss: 1232.1153 - acc: 0.0000e+0\n",
      "51200/95646 [==============>...............] - ETA: 15s - loss: 1218.2958 - acc: 0.0000e+0\n",
      "52352/95646 [===============>..............] - ETA: 14s - loss: 1199.3975 - acc: 0.0000e+0\n",
      "53504/95646 [===============>..............] - ETA: 14s - loss: 1189.0943 - acc: 0.0000e+0\n",
      "54784/95646 [================>.............] - ETA: 13s - loss: 1170.4500 - acc: 0.0000e+0\n",
      "56320/95646 [================>.............] - ETA: 13s - loss: 1170.3869 - acc: 0.0000e+0\n",
      "57728/95646 [=================>............] - ETA: 12s - loss: 1176.3759 - acc: 0.0000e+0\n",
      "58624/95646 [=================>............] - ETA: 12s - loss: 1168.0412 - acc: 0.0000e+0\n",
      "59776/95646 [=================>............] - ETA: 12s - loss: 1163.6899 - acc: 0.0000e+0\n",
      "60928/95646 [==================>...........] - ETA: 11s - loss: 1150.2180 - acc: 0.0000e+0\n",
      "62336/95646 [==================>...........] - ETA: 11s - loss: 1150.2570 - acc: 0.0000e+0\n",
      "63872/95646 [==================>...........] - ETA: 10s - loss: 1162.3061 - acc: 0.0000e+0\n",
      "64512/95646 [===================>..........] - ETA: 10s - loss: 1153.8704 - acc: 0.0000e+0\n",
      "65920/95646 [===================>..........] - ETA: 10s - loss: 1142.6808 - acc: 0.0000e+0\n",
      "66944/95646 [===================>..........] - ETA: 9s - loss: 1140.0347 - acc: 0.0000e+0\n",
      "68352/95646 [====================>.........] - ETA: 9s - loss: 1168.6578 - acc: 0.0000e+0\n",
      "69888/95646 [====================>.........] - ETA: 8s - loss: 1156.5943 - acc: 0.0000e+\n",
      "70784/95646 [=====================>........] - ETA: 8s - loss: 1147.2464 - acc: 0.0000e+\n",
      "72064/95646 [=====================>........] - ETA: 7s - loss: 1218.7461 - acc: 0.0000e+\n",
      "73600/95646 [======================>.......] - ETA: 7s - loss: 1228.3597 - acc: 0.0000e+\n",
      "74752/95646 [======================>.......] - ETA: 7s - loss: 1218.9509 - acc: 0.0000e+0\n",
      "76288/95646 [======================>.......] - ETA: 6s - loss: 1207.9235 - acc: 0.0000e+0\n",
      "77696/95646 [=======================>......] - ETA: 6s - loss: 1202.8910 - acc: 0.0000e+\n",
      "78976/95646 [=======================>......] - ETA: 5s - loss: 1198.7860 - acc: 0.0000e+\n",
      "80512/95646 [========================>.....] - ETA: 5s - loss: 1187.3530 - acc: 0.0000e+\n",
      "82048/95646 [========================>.....] - ETA: 4s - loss: 1188.2064 - acc: 0.0000e+\n",
      "83200/95646 [=========================>....] - ETA: 4s - loss: 1177.1237 - acc: 0.0000e+0\n",
      "84352/95646 [=========================>....] - ETA: 3s - loss: 1169.0660 - acc: 0.0000e+\n",
      "85888/95646 [=========================>....] - ETA: 3s - loss: 1161.5581 - acc: 0.0000e+\n",
      "86912/95646 [==========================>...] - ETA: 2s - loss: 1183.9991 - acc: 0.0000e+\n",
      "88320/95646 [==========================>...] - ETA: 2s - loss: 1176.3993 - acc: 0.0000e+\n",
      "89472/95646 [==========================>...] - ETA: 2s - loss: 1171.2887 - acc: 0.0000e+\n",
      "90752/95646 [===========================>..] - ETA: 1s - loss: 1163.3131 - acc: 0.0000e+0\n",
      "92160/95646 [===========================>..] - ETA: 1s - loss: 1162.2462 - acc: 0.0000e+\n",
      "93696/95646 [============================>.] - ETA: 0s - loss: 1157.6135 - acc: 0.0000e+\n",
      "94848/95646 [============================>.] - ETA: 0s - loss: 1152.3811 - acc: 0.0000e+\n",
      "95646/95646 [==============================] - 32s 334us/sample - loss: 1148.2210 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      " 1152/95646 [..............................] - ETA: 30s - loss: 616.3183 - acc: 0.0000e+00\n",
      " 2688/95646 [..............................] - ETA: 27s - loss: 667.2749 - acc: 0.0000e+\n",
      " 3968/95646 [>.............................] - ETA: 27s - loss: 745.5048 - acc: 0.0000e+\n",
      " 5504/95646 [>.............................] - ETA: 26s - loss: 764.9870 - acc: 0.0000e+\n",
      " 6912/95646 [=>............................] - ETA: 26s - loss: 829.2693 - acc: 0.0000e+\n",
      " 8192/95646 [=>............................] - ETA: 25s - loss: 866.3108 - acc: 0.0000e+0\n",
      " 9728/95646 [==>...........................] - ETA: 25s - loss: 890.2015 - acc: 0.0000e+0\n",
      "11264/95646 [==>...........................] - ETA: 24s - loss: 1282.1900 - acc: 0.0000e+0\n",
      "12544/95646 [==>...........................] - ETA: 24s - loss: 1267.8161 - acc: 0.0000e+0\n",
      "14080/95646 [===>..........................] - ETA: 24s - loss: 1202.9953 - acc: 0.0000e+0\n",
      "15360/95646 [===>..........................] - ETA: 23s - loss: 1201.9545 - acc: 0.0000e+0\n",
      "16896/95646 [====>.........................] - ETA: 23s - loss: 1239.7477 - acc: 0.0000e+0\n",
      "18048/95646 [====>.........................] - ETA: 23s - loss: 1226.8038 - acc: 0.0000e+0\n",
      "18944/95646 [====>.........................] - ETA: 23s - loss: 1183.8256 - acc: 0.0000e+0\n",
      "20352/95646 [=====>........................] - ETA: 23s - loss: 1153.7637 - acc: 0.0000e+0\n",
      "21632/95646 [=====>........................] - ETA: 22s - loss: 1139.9906 - acc: 0.0000e+0\n",
      "23168/95646 [======>.......................] - ETA: 22s - loss: 1101.4870 - acc: 0.0000e+0\n",
      "24704/95646 [======>.......................] - ETA: 21s - loss: 1085.4483 - acc: 0.0000e+0\n",
      "25984/95646 [=======>......................] - ETA: 21s - loss: 1049.9965 - acc: 0.0000e+0\n",
      "27136/95646 [=======>......................] - ETA: 21s - loss: 1036.8871 - acc: 0.0000e+0\n",
      "27904/95646 [=======>......................] - ETA: 21s - loss: 1091.9466 - acc: 0.0000e+0\n",
      "29440/95646 [========>.....................] - ETA: 20s - loss: 1092.3137 - acc: 0.0000e+0\n",
      "30720/95646 [========>.....................] - ETA: 20s - loss: 1085.5940 - acc: 0.0000e+0\n",
      "31872/95646 [========>.....................] - ETA: 20s - loss: 1130.7004 - acc: 0.0000e+0\n",
      "33408/95646 [=========>....................] - ETA: 19s - loss: 1109.1987 - acc: 0.0000e+0\n",
      "34688/95646 [=========>....................] - ETA: 19s - loss: 1086.6329 - acc: 0.0000e+0\n",
      "36224/95646 [==========>...................] - ETA: 18s - loss: 1063.4816 - acc: 0.0000e+0\n",
      "37632/95646 [==========>...................] - ETA: 18s - loss: 1051.1593 - acc: 0.0000e+0\n",
      "38784/95646 [===========>..................] - ETA: 17s - loss: 1038.8676 - acc: 0.0000e+0\n",
      "40320/95646 [===========>..................] - ETA: 17s - loss: 1051.0103 - acc: 0.0000e+0\n",
      "41472/95646 [============>.................] - ETA: 16s - loss: 1034.0693 - acc: 0.0000e+0\n",
      "43008/95646 [============>.................] - ETA: 16s - loss: 1012.7268 - acc: 0.0000e+0\n",
      "44416/95646 [============>.................] - ETA: 16s - loss: 999.7620 - acc: 0.0000e+00\n",
      "45568/95646 [=============>................] - ETA: 15s - loss: 984.7466 - acc: 0.0000e+0\n",
      "47104/95646 [=============>................] - ETA: 15s - loss: 973.5354 - acc: 0.0000e+\n",
      "48384/95646 [==============>...............] - ETA: 14s - loss: 976.6017 - acc: 0.0000e+\n",
      "49664/95646 [==============>...............] - ETA: 14s - loss: 961.6846 - acc: 0.0000e+\n",
      "51072/95646 [===============>..............] - ETA: 13s - loss: 948.8626 - acc: 0.0000e+\n",
      "52352/95646 [===============>..............] - ETA: 13s - loss: 944.1331 - acc: 0.0000e+\n",
      "53632/95646 [===============>..............] - ETA: 12s - loss: 932.7841 - acc: 0.0000e+0\n",
      "55168/95646 [================>.............] - ETA: 12s - loss: 932.7665 - acc: 0.0000e+\n",
      "56576/95646 [================>.............] - ETA: 12s - loss: 925.6974 - acc: 0.0000e+\n",
      "57472/95646 [=================>............] - ETA: 11s - loss: 918.3741 - acc: 0.0000e+\n",
      "58752/95646 [=================>............] - ETA: 11s - loss: 913.9812 - acc: 0.0000e+\n",
      "60904/95646 [=================>............] - ETA: 11s - loss: 910.3505 - acc: 0.0000e+\n",
      "61312/95646 [==================>...........] - ETA: 10s - loss: 905.1338 - acc: 0.0000e+0\n",
      "62848/95646 [==================>...........] - ETA: 10s - loss: 918.0243 - acc: 0.0000e+\n",
      "64120/95646 [===================>..........] - ETA: 9s - loss: 913.8725 - acc: 0.0000e+00\n",
      "65152/95646 [===================>..........] - ETA: 9s - loss: 916.9663 - acc: 0.0000e+0\n",
      "66304/95646 [===================>..........] - ETA: 9s - loss: 908.3128 - acc: 0.0000e+0\n",
      "67840/95646 [====================>.........] - ETA: 8s - loss: 915.6273 - acc: 0.0000e+0\n",
      "69120/95646 [====================>.........] - ETA: 8s - loss: 917.5957 - acc: 0.0000e+0\n",
      "70144/95646 [=====================>........] - ETA: 7s - loss: 909.2785 - acc: 0.0000e+0\n",
      "71680/95646 [=====================>........] - ETA: 7s - loss: 903.1021 - acc: 0.0000e+0\n",
      "73216/95646 [=====================>........] - ETA: 7s - loss: 904.6232 - acc: 0.0000e+0\n",
      "74368/95646 [======================>.......] - ETA: 6s - loss: 901.0448 - acc: 0.0000e+0\n",
      "75904/95646 [======================>.......] - ETA: 6s - loss: 893.0940 - acc: 0.0000e+0\n",
      "77440/95646 [=======================>......] - ETA: 5s - loss: 887.1018 - acc: 0.0000e+0\n",
      "78592/95646 [=======================>......] - ETA: 5s - loss: 881.8728 - acc: 0.0000e+0\n",
      "80128/95646 [========================>.....] - ETA: 4s - loss: 873.1420 - acc: 0.0000e+0\n",
      "81408/95646 [========================>.....] - ETA: 4s - loss: 867.9368 - acc: 0.0000e+0\n",
      "82944/95646 [========================>.....] - ETA: 4s - loss: 863.6860 - acc: 0.0000e+0\n",
      "83968/95646 [=========================>....] - ETA: 3s - loss: 857.6044 - acc: 0.0000e+0\n",
      "85504/95646 [=========================>....] - ETA: 3s - loss: 850.5526 - acc: 0.0000e+0\n",
      "86912/95646 [==========================>...] - ETA: 2s - loss: 849.0911 - acc: 0.0000e+0\n",
      "88064/95646 [==========================>...] - ETA: 2s - loss: 891.0120 - acc: 0.0000e+0\n",
      "89472/95646 [===========================>..] - ETA: 1s - loss: 888.9212 - acc: 0.0000e+0\n",
      "90752/95646 [===========================>..] - ETA: 1s - loss: 890.2203 - acc: 0.0000e+0\n",
      "91904/95646 [===========================>..] - ETA: 1s - loss: 884.8589 - acc: 0.0000e+0\n",
      "93184/95646 [============================>.] - ETA: 0s - loss: 896.3788 - acc: 0.0000e+0\n",
      "94720/95646 [============================>.] - ETA: 0s - loss: 904.7553 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 309us/sample - loss: 900.6120 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "  896/95646 [..............................] - ETA: 41s - loss: 370.2660 - acc: 0.0000e+\n",
      " 2304/95646 [..............................] - ETA: 32s - loss: 540.4709 - acc: 0.0000e+\n",
      " 3584/95646 [>.............................] - ETA: 29s - loss: 539.1910 - acc: 0.0000e+0\n",
      " 4992/95646 [>.............................] - ETA: 28s - loss: 562.9049 - acc: 0.0000e+0\n",
      " 6528/95646 [=>............................] - ETA: 27s - loss: 598.7405 - acc: 0.0000e+\n",
      " 7680/95646 [=>............................] - ETA: 26s - loss: 742.1619 - acc: 0.0000e+\n",
      " 9088/95646 [=>............................] - ETA: 26s - loss: 834.2829 - acc: 0.0000e+\n",
      "10496/95646 [==>...........................] - ETA: 25s - loss: 827.5941 - acc: 0.0000e+\n",
      "11776/95646 [==>...........................] - ETA: 25s - loss: 796.4725 - acc: 0.0000e+0\n",
      "13184/95646 [===>..........................] - ETA: 24s - loss: 753.2029 - acc: 0.0000e+0\n",
      "14464/95646 [===>..........................] - ETA: 24s - loss: 727.4947 - acc: 0.0000e+\n",
      "15744/95646 [===>..........................] - ETA: 24s - loss: 720.7091 - acc: 0.0000e+\n",
      "17280/95646 [====>.........................] - ETA: 23s - loss: 726.5855 - acc: 0.0000e+\n",
      "18816/95646 [====>.........................] - ETA: 23s - loss: 717.9660 - acc: 0.0000e+\n",
      "19456/95646 [=====>........................] - ETA: 23s - loss: 701.4594 - acc: 0.0000e+0\n",
      "20864/95646 [=====>........................] - ETA: 22s - loss: 700.8482 - acc: 0.0000e+0\n",
      "22272/95646 [=====>........................] - ETA: 22s - loss: 685.0739 - acc: 0.0000e+\n",
      "23552/95646 [======>.......................] - ETA: 21s - loss: 671.1479 - acc: 0.0000e+\n",
      "24960/95646 [======>.......................] - ETA: 21s - loss: 661.7592 - acc: 0.0000e+\n",
      "26368/95646 [=======>......................] - ETA: 21s - loss: 672.3989 - acc: 0.0000e+\n",
      "27648/95646 [=======>......................] - ETA: 21s - loss: 710.1681 - acc: 0.0000e+0\n",
      "29056/95646 [========>.....................] - ETA: 20s - loss: 710.4930 - acc: 0.0000e+\n",
      "30592/95646 [========>.....................] - ETA: 20s - loss: 705.8420 - acc: 0.0000e+\n",
      "31872/95646 [========>.....................] - ETA: 19s - loss: 699.1042 - acc: 0.0000e+\n",
      "33408/95646 [=========>....................] - ETA: 18s - loss: 687.6493 - acc: 0.0000e+\n",
      "34816/95646 [=========>....................] - ETA: 18s - loss: 682.8244 - acc: 0.0000e+\n",
      "36096/95646 [==========>...................] - ETA: 18s - loss: 677.8179 - acc: 0.0000e+0\n",
      "37504/95646 [==========>...................] - ETA: 17s - loss: 667.1936 - acc: 0.0000e+\n",
      "39040/95646 [===========>..................] - ETA: 17s - loss: 661.6730 - acc: 0.0000e+\n",
      "39936/95646 [===========>..................] - ETA: 17s - loss: 659.1706 - acc: 0.0000e+\n",
      "41472/95646 [============>.................] - ETA: 16s - loss: 652.3352 - acc: 0.0000e+\n",
      "43008/95646 [============>.................] - ETA: 16s - loss: 643.1669 - acc: 0.0000e+\n",
      "44288/95646 [============>.................] - ETA: 15s - loss: 643.8107 - acc: 0.0000e+0\n",
      "45824/95646 [=============>................] - ETA: 15s - loss: 640.1937 - acc: 0.0000e+\n",
      "46848/95646 [=============>................] - ETA: 14s - loss: 638.3611 - acc: 0.0000e+\n",
      "47872/95646 [==============>...............] - ETA: 14s - loss: 636.9422 - acc: 0.0000e+\n",
      "49152/95646 [==============>...............] - ETA: 14s - loss: 633.8684 - acc: 0.0000e+\n",
      "49792/95646 [==============>...............] - ETA: 14s - loss: 629.6543 - acc: 0.0000e+0\n",
      "51200/95646 [===============>..............] - ETA: 13s - loss: 627.8139 - acc: 0.0000e+0\n",
      "51968/95646 [===============>..............] - ETA: 13s - loss: 629.3465 - acc: 0.0000e+\n",
      "53120/95646 [===============>..............] - ETA: 13s - loss: 634.7730 - acc: 0.0000e+\n",
      "54400/95646 [================>.............] - ETA: 13s - loss: 704.0994 - acc: 0.0000e+\n",
      "55552/95646 [================>.............] - ETA: 12s - loss: 704.8914 - acc: 0.0000e+\n",
      "56448/95646 [================>.............] - ETA: 12s - loss: 698.0967 - acc: 0.0000e+0\n",
      "57344/95646 [================>.............] - ETA: 12s - loss: 698.0563 - acc: 0.0000e+0\n",
      "58240/95646 [=================>............] - ETA: 12s - loss: 709.2457 - acc: 0.0000e+\n",
      "59392/95646 [=================>............] - ETA: 11s - loss: 704.9874 - acc: 0.0000e+\n",
      "60928/95646 [==================>...........] - ETA: 11s - loss: 702.7225 - acc: 0.0000e+\n",
      "61696/95646 [==================>...........] - ETA: 11s - loss: 701.7727 - acc: 0.0000e+\n",
      "62464/95646 [==================>...........] - ETA: 10s - loss: 695.6241 - acc: 0.0000e+0\n",
      "63616/95646 [==================>...........] - ETA: 10s - loss: 705.7007 - acc: 0.0000e+0\n",
      "65152/95646 [===================>..........] - ETA: 10s - loss: 707.0504 - acc: 0.0000e+\n",
      "66176/95646 [===================>..........] - ETA: 9s - loss: 714.1991 - acc: 0.0000e+00\n",
      "67456/95646 [====================>.........] - ETA: 9s - loss: 716.5776 - acc: 0.0000e+0\n",
      "68608/95646 [====================>.........] - ETA: 8s - loss: 714.5217 - acc: 0.0000e+0\n",
      "70144/95646 [====================>.........] - ETA: 8s - loss: 740.3351 - acc: 0.0000e+0\n",
      "71296/95646 [=====================>........] - ETA: 8s - loss: 735.5562 - acc: 0.0000e+0\n",
      "72832/95646 [=====================>........] - ETA: 7s - loss: 728.6974 - acc: 0.0000e+0\n",
      "74368/95646 [======================>.......] - ETA: 7s - loss: 721.2086 - acc: 0.0000e+0\n",
      "75648/95646 [======================>.......] - ETA: 6s - loss: 717.7378 - acc: 0.0000e+0\n",
      "77184/95646 [=======================>......] - ETA: 6s - loss: 711.0182 - acc: 0.0000e+0\n",
      "78720/95646 [=======================>......] - ETA: 5s - loss: 712.6410 - acc: 0.0000e+0\n",
      "80256/95646 [========================>.....] - ETA: 5s - loss: 715.4220 - acc: 0.0000e+0\n",
      "81408/95646 [========================>.....] - ETA: 4s - loss: 709.9002 - acc: 0.0000e+0\n",
      "82688/95646 [========================>.....] - ETA: 4s - loss: 709.2950 - acc: 0.0000e+0\n",
      "84096/95646 [=========================>....] - ETA: 3s - loss: 710.1220 - acc: 0.0000e+0\n",
      "85248/95646 [=========================>....] - ETA: 3s - loss: 709.1006 - acc: 0.0000e+0\n",
      "86272/95646 [==========================>...] - ETA: 3s - loss: 706.4896 - acc: 0.0000e+0\n",
      "87552/95646 [==========================>...] - ETA: 2s - loss: 700.1983 - acc: 0.0000e+0\n",
      "88832/95646 [==========================>...] - ETA: 2s - loss: 713.2458 - acc: 0.0000e+0\n",
      "89984/95646 [===========================>..] - ETA: 1s - loss: 708.8016 - acc: 0.0000e+0\n",
      "91520/95646 [===========================>..] - ETA: 1s - loss: 705.7281 - acc: 0.0000e+0\n",
      "93056/95646 [============================>.] - ETA: 0s - loss: 753.3936 - acc: 0.0000e+0\n",
      "94208/95646 [============================>.] - ETA: 0s - loss: 748.1147 - acc: 0.0000e+0\n",
      "95488/95646 [============================>.] - ETA: 0s - loss: 753.9359 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 31s 325us/sample - loss: 753.8107 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      " 1280/95646 [..............................] - ETA: 26s - loss: 3191.3232 - acc: 0.0000e+0\n",
      " 2816/95646 [..............................] - ETA: 25s - loss: 1844.1810 - acc: 0.0000e+0\n",
      " 3712/95646 [>.............................] - ETA: 27s - loss: 1850.5232 - acc: 0.0000e+0\n",
      " 5248/95646 [>.............................] - ETA: 27s - loss: 1440.3654 - acc: 0.0000e+0\n",
      " 6784/95646 [=>............................] - ETA: 26s - loss: 1256.9498 - acc: 0.0000e+0\n",
      " 8064/95646 [=>............................] - ETA: 25s - loss: 1104.0473 - acc: 0.0000e+0\n",
      " 9600/95646 [==>...........................] - ETA: 25s - loss: 1023.3365 - acc: 0.0000e+0\n",
      "10752/95646 [==>...........................] - ETA: 24s - loss: 933.8779 - acc: 0.0000e+0\n",
      "12288/95646 [==>...........................] - ETA: 24s - loss: 861.4447 - acc: 0.0000e+\n",
      "13824/95646 [===>..........................] - ETA: 24s - loss: 812.9570 - acc: 0.0000e+\n",
      "14976/95646 [===>..........................] - ETA: 23s - loss: 821.7830 - acc: 0.0000e+0\n",
      "16256/95646 [====>.........................] - ETA: 23s - loss: 787.0241 - acc: 0.0000e+0\n",
      "17408/95646 [====>.........................] - ETA: 23s - loss: 768.4867 - acc: 0.0000e+\n",
      "18560/95646 [====>.........................] - ETA: 23s - loss: 742.8935 - acc: 0.0000e+\n",
      "20096/95646 [=====>........................] - ETA: 22s - loss: 765.1266 - acc: 0.0000e+\n",
      "21504/95646 [=====>........................] - ETA: 22s - loss: 742.8380 - acc: 0.0000e+\n",
      "22528/95646 [======>.......................] - ETA: 22s - loss: 729.2074 - acc: 0.0000e+0\n",
      "23680/95646 [======>.......................] - ETA: 21s - loss: 712.1054 - acc: 0.0000e+\n",
      "24960/95646 [======>.......................] - ETA: 21s - loss: 707.2293 - acc: 0.0000e+\n",
      "26112/95646 [=======>......................] - ETA: 21s - loss: 688.1090 - acc: 0.0000e+\n",
      "27648/95646 [=======>......................] - ETA: 20s - loss: 669.4263 - acc: 0.0000e+\n",
      "29184/95646 [========>.....................] - ETA: 20s - loss: 654.5342 - acc: 0.0000e+\n",
      "30336/95646 [========>.....................] - ETA: 20s - loss: 649.1913 - acc: 0.0000e+0\n",
      "31872/95646 [========>.....................] - ETA: 19s - loss: 639.6709 - acc: 0.0000e+\n",
      "33280/95646 [=========>....................] - ETA: 19s - loss: 628.8234 - acc: 0.0000e+\n",
      "34560/95646 [=========>....................] - ETA: 18s - loss: 623.1860 - acc: 0.0000e+\n",
      "36096/95646 [==========>...................] - ETA: 18s - loss: 610.9875 - acc: 0.0000e+\n",
      "37248/95646 [==========>...................] - ETA: 17s - loss: 603.0274 - acc: 0.0000e+\n",
      "38656/95646 [===========>..................] - ETA: 17s - loss: 608.7576 - acc: 0.0000e+0\n",
      "40192/95646 [===========>..................] - ETA: 16s - loss: 731.6075 - acc: 0.0000e+\n",
      "41472/95646 [============>.................] - ETA: 16s - loss: 717.4711 - acc: 0.0000e+\n",
      "43008/95646 [============>.................] - ETA: 15s - loss: 704.6953 - acc: 0.0000e+\n",
      "44160/95646 [============>.................] - ETA: 15s - loss: 697.4306 - acc: 0.0000e+\n",
      "45440/95646 [=============>................] - ETA: 15s - loss: 688.5315 - acc: 0.0000e+0\n",
      "46720/95646 [=============>................] - ETA: 14s - loss: 684.2268 - acc: 0.0000e+0\n",
      "48256/95646 [==============>...............] - ETA: 14s - loss: 686.6102 - acc: 0.0000e+\n",
      "49536/95646 [==============>...............] - ETA: 13s - loss: 693.1339 - acc: 0.0000e+\n",
      "51072/95646 [===============>..............] - ETA: 13s - loss: 689.9097 - acc: 0.0000e+\n",
      "52224/95646 [===============>..............] - ETA: 13s - loss: 685.3675 - acc: 0.0000e+\n",
      "53504/95646 [===============>..............] - ETA: 12s - loss: 680.8137 - acc: 0.0000e+0\n",
      "55040/95646 [================>.............] - ETA: 12s - loss: 672.2594 - acc: 0.0000e+0\n",
      "56448/95646 [================>.............] - ETA: 11s - loss: 664.8999 - acc: 0.0000e+\n",
      "57472/95646 [=================>............] - ETA: 11s - loss: 664.3293 - acc: 0.0000e+\n",
      "58752/95646 [=================>............] - ETA: 11s - loss: 668.1461 - acc: 0.0000e+\n",
      "60288/95646 [=================>............] - ETA: 10s - loss: 663.3311 - acc: 0.0000e+\n",
      "61568/95646 [==================>...........] - ETA: 10s - loss: 656.7680 - acc: 0.0000e+0\n",
      "62976/95646 [==================>...........] - ETA: 9s - loss: 652.0428 - acc: 0.0000e+00\n",
      "64512/95646 [===================>..........] - ETA: 9s - loss: 649.6048 - acc: 0.0000e+0\n",
      "65408/95646 [===================>..........] - ETA: 9s - loss: 644.1447 - acc: 0.0000e+0\n",
      "66944/95646 [===================>..........] - ETA: 8s - loss: 636.0540 - acc: 0.0000e+0\n",
      "68480/95646 [====================>.........] - ETA: 8s - loss: 630.1154 - acc: 0.0000e+0\n",
      "69888/95646 [====================>.........] - ETA: 7s - loss: 624.7861 - acc: 0.0000e+0\n",
      "71168/95646 [=====================>........] - ETA: 7s - loss: 633.7565 - acc: 0.0000e+0\n",
      "72704/95646 [=====================>........] - ETA: 6s - loss: 629.2537 - acc: 0.0000e+0\n",
      "74112/95646 [======================>.......] - ETA: 6s - loss: 625.1365 - acc: 0.0000e+0\n",
      "75392/95646 [======================>.......] - ETA: 6s - loss: 622.2751 - acc: 0.0000e+0\n",
      "76800/95646 [=======================>......] - ETA: 5s - loss: 636.1943 - acc: 0.0000e+0\n",
      "77952/95646 [=======================>......] - ETA: 5s - loss: 633.2101 - acc: 0.0000e+0\n",
      "79360/95646 [=======================>......] - ETA: 4s - loss: 629.7515 - acc: 0.0000e+0\n",
      "80640/95646 [========================>.....] - ETA: 4s - loss: 636.6439 - acc: 0.0000e+0\n",
      "82176/95646 [========================>.....] - ETA: 4s - loss: 632.8112 - acc: 0.0000e+0\n",
      "83456/95646 [=========================>....] - ETA: 3s - loss: 629.2622 - acc: 0.0000e+0\n",
      "84992/95646 [=========================>....] - ETA: 3s - loss: 629.0172 - acc: 0.0000e+0\n",
      "86272/95646 [==========================>...] - ETA: 2s - loss: 634.7285 - acc: 0.0000e+0\n",
      "87552/95646 [==========================>...] - ETA: 2s - loss: 632.2178 - acc: 0.0000e+0\n",
      "88704/95646 [==========================>...] - ETA: 2s - loss: 630.7224 - acc: 0.0000e+0\n",
      "89344/95646 [===========================>..] - ETA: 1s - loss: 627.8371 - acc: 0.0000e+0\n",
      "90368/95646 [===========================>..] - ETA: 1s - loss: 624.4099 - acc: 0.0000e+0\n",
      "91520/95646 [===========================>..] - ETA: 1s - loss: 622.6765 - acc: 0.0000e+0\n",
      "92928/95646 [============================>.] - ETA: 0s - loss: 622.3185 - acc: 0.0000e+0\n",
      "93696/95646 [============================>.] - ETA: 0s - loss: 619.5351 - acc: 0.0000e+0\n",
      "95232/95646 [============================>.] - ETA: 0s - loss: 617.8075 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 314us/sample - loss: 616.4767 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      " 1280/95646 [..............................] - ETA: 35s - loss: 265.5752 - acc: 0.0000e+\n",
      " 2688/95646 [..............................] - ETA: 32s - loss: 250.3283 - acc: 0.0000e+\n",
      " 3456/95646 [>.............................] - ETA: 34s - loss: 289.7178 - acc: 0.0000e+0\n",
      " 4608/95646 [>.............................] - ETA: 34s - loss: 299.4080 - acc: 0.0000e+0\n",
      " 5760/95646 [>.............................] - ETA: 34s - loss: 383.4743 - acc: 0.0000e+\n",
      " 6784/95646 [=>............................] - ETA: 33s - loss: 383.4959 - acc: 0.0000e+\n",
      " 8192/95646 [=>............................] - ETA: 32s - loss: 406.6823 - acc: 0.0000e+\n",
      " 9600/95646 [==>...........................] - ETA: 31s - loss: 382.6833 - acc: 0.0000e+\n",
      "10624/95646 [==>...........................] - ETA: 30s - loss: 460.7424 - acc: 0.0000e+0\n",
      "12160/95646 [==>...........................] - ETA: 29s - loss: 486.9407 - acc: 0.0000e+0\n",
      "13568/95646 [===>..........................] - ETA: 28s - loss: 471.4597 - acc: 0.0000e+\n",
      "14592/95646 [===>..........................] - ETA: 27s - loss: 460.9666 - acc: 0.0000e+\n",
      "16128/95646 [====>.........................] - ETA: 27s - loss: 439.3301 - acc: 0.0000e+\n",
      "17536/95646 [====>.........................] - ETA: 26s - loss: 438.8654 - acc: 0.0000e+\n",
      "18816/95646 [====>.........................] - ETA: 25s - loss: 424.6322 - acc: 0.0000e+0\n",
      "20096/95646 [=====>........................] - ETA: 25s - loss: 427.3468 - acc: 0.0000e+0\n",
      "21248/95646 [=====>........................] - ETA: 24s - loss: 431.5547 - acc: 0.0000e+\n",
      "22400/95646 [======>.......................] - ETA: 24s - loss: 431.2836 - acc: 0.0000e+\n",
      "23808/95646 [======>.......................] - ETA: 23s - loss: 441.1199 - acc: 0.0000e+\n",
      "25344/95646 [======>.......................] - ETA: 23s - loss: 443.6481 - acc: 0.0000e+\n",
      "26624/95646 [=======>......................] - ETA: 22s - loss: 439.5188 - acc: 0.0000e+0\n",
      "28160/95646 [=======>......................] - ETA: 22s - loss: 478.8488 - acc: 0.0000e+\n",
      "29184/95646 [========>.....................] - ETA: 21s - loss: 472.2445 - acc: 0.0000e+\n",
      "30336/95646 [========>.....................] - ETA: 21s - loss: 466.6179 - acc: 0.0000e+\n",
      "31744/95646 [========>.....................] - ETA: 20s - loss: 465.1049 - acc: 0.0000e+\n",
      "33280/95646 [=========>....................] - ETA: 20s - loss: 457.6448 - acc: 0.0000e+\n",
      "34304/95646 [=========>....................] - ETA: 19s - loss: 466.8423 - acc: 0.0000e+0\n",
      "35840/95646 [==========>...................] - ETA: 19s - loss: 460.7129 - acc: 0.0000e+\n",
      "37376/95646 [==========>...................] - ETA: 18s - loss: 457.6542 - acc: 0.0000e+\n",
      "38656/95646 [===========>..................] - ETA: 18s - loss: 475.9103 - acc: 0.0000e+\n",
      "40192/95646 [===========>..................] - ETA: 17s - loss: 468.8140 - acc: 0.0000e+\n",
      "41344/95646 [===========>..................] - ETA: 17s - loss: 465.1113 - acc: 0.0000e+\n",
      "42496/95646 [============>.................] - ETA: 16s - loss: 463.4528 - acc: 0.0000e+0\n",
      "44032/95646 [============>.................] - ETA: 16s - loss: 457.3278 - acc: 0.0000e+\n",
      "45312/95646 [=============>................] - ETA: 15s - loss: 452.8625 - acc: 0.0000e+\n",
      "46848/95646 [=============>................] - ETA: 15s - loss: 451.2886 - acc: 0.0000e+\n",
      "48000/95646 [==============>...............] - ETA: 15s - loss: 451.0602 - acc: 0.0000e+\n",
      "49280/95646 [==============>...............] - ETA: 14s - loss: 450.6503 - acc: 0.0000e+0\n",
      "50816/95646 [==============>...............] - ETA: 14s - loss: 460.4230 - acc: 0.0000e+0\n",
      "52352/95646 [===============>..............] - ETA: 13s - loss: 460.4554 - acc: 0.0000e+\n",
      "53632/95646 [===============>..............] - ETA: 13s - loss: 460.8582 - acc: 0.0000e+\n",
      "55040/95646 [================>.............] - ETA: 12s - loss: 457.0822 - acc: 0.0000e+\n",
      "56448/95646 [================>.............] - ETA: 12s - loss: 455.1499 - acc: 0.0000e+\n",
      "57728/95646 [=================>............] - ETA: 11s - loss: 455.8927 - acc: 0.0000e+0\n",
      "59264/95646 [=================>............] - ETA: 11s - loss: 453.0564 - acc: 0.0000e+0\n",
      "60672/95646 [==================>...........] - ETA: 10s - loss: 450.1231 - acc: 0.0000e+\n",
      "61824/95646 [==================>...........] - ETA: 10s - loss: 446.2001 - acc: 0.0000e+\n",
      "63232/95646 [==================>...........] - ETA: 10s - loss: 442.0562 - acc: 0.0000e+\n",
      "64768/95646 [===================>..........] - ETA: 9s - loss: 438.8587 - acc: 0.0000e+00\n",
      "66048/95646 [===================>..........] - ETA: 9s - loss: 436.4020 - acc: 0.0000e+0\n",
      "67456/95646 [====================>.........] - ETA: 8s - loss: 446.5810 - acc: 0.0000e+0\n",
      "68864/95646 [====================>.........] - ETA: 8s - loss: 442.6096 - acc: 0.0000e+0\n",
      "70400/95646 [=====================>........] - ETA: 7s - loss: 495.6436 - acc: 0.0000e+0\n",
      "71680/95646 [=====================>........] - ETA: 7s - loss: 496.6795 - acc: 0.0000e+0\n",
      "73216/95646 [=====================>........] - ETA: 6s - loss: 495.9719 - acc: 0.0000e+0\n",
      "74624/95646 [======================>.......] - ETA: 6s - loss: 491.7435 - acc: 0.0000e+0\n",
      "75904/95646 [======================>.......] - ETA: 6s - loss: 491.9184 - acc: 0.0000e+0\n",
      "77440/95646 [=======================>......] - ETA: 5s - loss: 488.8230 - acc: 0.0000e+0\n",
      "78976/95646 [=======================>......] - ETA: 5s - loss: 532.7391 - acc: 0.0000e+0\n",
      "80512/95646 [========================>.....] - ETA: 4s - loss: 527.8816 - acc: 0.0000e+0\n",
      "81536/95646 [========================>.....] - ETA: 4s - loss: 523.9361 - acc: 0.0000e+0\n",
      "82816/95646 [========================>.....] - ETA: 3s - loss: 526.0093 - acc: 0.0000e+0\n",
      "84352/95646 [=========================>....] - ETA: 3s - loss: 521.7216 - acc: 0.0000e+0\n",
      "85760/95646 [=========================>....] - ETA: 3s - loss: 519.0713 - acc: 0.0000e+0\n",
      "86784/95646 [==========================>...] - ETA: 2s - loss: 516.1994 - acc: 0.0000e+0\n",
      "87936/95646 [==========================>...] - ETA: 2s - loss: 513.4925 - acc: 0.0000e+0\n",
      "89344/95646 [==========================>...] - ETA: 1s - loss: 509.3312 - acc: 0.0000e+0\n",
      "90624/95646 [===========================>..] - ETA: 1s - loss: 507.1733 - acc: 0.0000e+0\n",
      "92160/95646 [===========================>..] - ETA: 1s - loss: 503.9035 - acc: 0.0000e+0\n",
      "93696/95646 [============================>.] - ETA: 0s - loss: 500.1206 - acc: 0.0000e+0\n",
      "95104/95646 [============================>.] - ETA: 0s - loss: 497.6892 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 310us/sample - loss: 495.9778 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      " 1408/95646 [..............................] - ETA: 26s - loss: 1177.9305 - acc: 0.0000e+0\n",
      " 2944/95646 [..............................] - ETA: 26s - loss: 734.6071 - acc: 0.0000e+00\n",
      " 4224/95646 [>.............................] - ETA: 25s - loss: 551.0366 - acc: 0.0000e+0\n",
      " 5376/95646 [>.............................] - ETA: 27s - loss: 511.0099 - acc: 0.0000e+0\n",
      " 6912/95646 [=>............................] - ETA: 26s - loss: 455.5533 - acc: 0.0000e+\n",
      " 8192/95646 [=>............................] - ETA: 25s - loss: 413.7942 - acc: 0.0000e+\n",
      " 9600/95646 [==>...........................] - ETA: 25s - loss: 384.8949 - acc: 0.0000e+\n",
      "11136/95646 [==>...........................] - ETA: 24s - loss: 362.0490 - acc: 0.0000e+\n",
      "12288/95646 [==>...........................] - ETA: 24s - loss: 416.0796 - acc: 0.0000e+0\n",
      "13696/95646 [===>..........................] - ETA: 24s - loss: 401.4829 - acc: 0.0000e+0\n",
      "15232/95646 [===>..........................] - ETA: 23s - loss: 400.0891 - acc: 0.0000e+\n",
      "16512/95646 [====>.........................] - ETA: 23s - loss: 389.1214 - acc: 0.0000e+\n",
      "17664/95646 [====>.........................] - ETA: 23s - loss: 382.9856 - acc: 0.0000e+\n",
      "18816/95646 [====>.........................] - ETA: 23s - loss: 373.2070 - acc: 0.0000e+\n",
      "20096/95646 [=====>........................] - ETA: 22s - loss: 371.7573 - acc: 0.0000e+0\n",
      "21504/95646 [=====>........................] - ETA: 22s - loss: 363.5352 - acc: 0.0000e+\n",
      "23040/95646 [======>.......................] - ETA: 21s - loss: 361.2050 - acc: 0.0000e+\n",
      "24192/95646 [======>.......................] - ETA: 21s - loss: 350.1924 - acc: 0.0000e+\n",
      "25472/95646 [======>.......................] - ETA: 21s - loss: 346.4263 - acc: 0.0000e+\n",
      "26624/95646 [=======>......................] - ETA: 21s - loss: 341.0539 - acc: 0.0000e+\n",
      "27904/95646 [=======>......................] - ETA: 20s - loss: 339.9816 - acc: 0.0000e+0\n",
      "29312/95646 [========>.....................] - ETA: 20s - loss: 341.5880 - acc: 0.0000e+\n",
      "30080/95646 [========>.....................] - ETA: 20s - loss: 340.0727 - acc: 0.0000e+\n",
      "30976/95646 [========>.....................] - ETA: 20s - loss: 339.0898 - acc: 0.0000e+\n",
      "32256/95646 [=========>....................] - ETA: 20s - loss: 442.4440 - acc: 0.0000e+\n",
      "33026/95646 [=========>....................] - ETA: 20s - loss: 439.2144 - acc: 0.0000e+\n",
      "33920/95646 [=========>....................] - ETA: 20s - loss: 431.2330 - acc: 0.0000e+0\n",
      "34944/95646 [=========>....................] - ETA: 20s - loss: 427.8017 - acc: 0.0000e+\n",
      "35712/95646 [==========>...................] - ETA: 19s - loss: 421.0899 - acc: 0.0000e+\n",
      "36992/95646 [==========>...................] - ETA: 19s - loss: 420.9727 - acc: 0.0000e+\n",
      "38016/95646 [==========>...................] - ETA: 19s - loss: 415.8209 - acc: 0.0000e+\n",
      "39296/95646 [===========>..................] - ETA: 18s - loss: 412.7020 - acc: 0.0000e+0\n",
      "40448/95646 [===========>..................] - ETA: 18s - loss: 407.0476 - acc: 0.0000e+0\n",
      "41856/95646 [============>.................] - ETA: 18s - loss: 401.7504 - acc: 0.0000e+\n",
      "42880/95646 [============>.................] - ETA: 17s - loss: 398.0622 - acc: 0.0000e+\n",
      "44288/95646 [============>.................] - ETA: 17s - loss: 392.7184 - acc: 0.0000e+\n",
      "45184/95646 [=============>................] - ETA: 17s - loss: 389.8303 - acc: 0.0000e+\n",
      "46080/95646 [=============>................] - ETA: 17s - loss: 390.1461 - acc: 0.0000e+0\n",
      "47360/95646 [=============>................] - ETA: 16s - loss: 385.3054 - acc: 0.0000e+0\n",
      "48768/95646 [==============>...............] - ETA: 16s - loss: 387.6812 - acc: 0.0000e+\n",
      "50048/95646 [==============>...............] - ETA: 15s - loss: 384.6059 - acc: 0.0000e+\n",
      "51200/95646 [===============>..............] - ETA: 15s - loss: 382.7916 - acc: 0.0000e+\n",
      "52608/95646 [===============>..............] - ETA: 14s - loss: 378.4786 - acc: 0.0000e+\n",
      "53888/95646 [===============>..............] - ETA: 14s - loss: 373.9176 - acc: 0.0000e+0\n",
      "55424/95646 [================>.............] - ETA: 13s - loss: 372.6886 - acc: 0.0000e+0\n",
      "56960/95646 [================>.............] - ETA: 13s - loss: 371.0088 - acc: 0.0000e+\n",
      "57984/95646 [=================>............] - ETA: 12s - loss: 366.9961 - acc: 0.0000e+\n",
      "59136/95646 [=================>............] - ETA: 12s - loss: 365.3397 - acc: 0.0000e+\n",
      "60416/95646 [=================>............] - ETA: 11s - loss: 361.7328 - acc: 0.0000e+\n",
      "61696/95646 [==================>...........] - ETA: 11s - loss: 367.4877 - acc: 0.0000e+0\n",
      "63104/95646 [==================>...........] - ETA: 10s - loss: 363.8876 - acc: 0.0000e+\n",
      "64512/95646 [===================>..........] - ETA: 10s - loss: 373.4551 - acc: 0.0000e+\n",
      "65664/95646 [===================>..........] - ETA: 10s - loss: 372.3386 - acc: 0.0000e+\n",
      "67072/95646 [====================>.........] - ETA: 9s - loss: 371.4252 - acc: 0.0000e+00\n",
      "68608/95646 [====================>.........] - ETA: 9s - loss: 369.0962 - acc: 0.0000e+0\n",
      "69888/95646 [====================>.........] - ETA: 8s - loss: 368.8803 - acc: 0.0000e+0\n",
      "71424/95646 [=====================>........] - ETA: 8s - loss: 410.9229 - acc: 0.0000e+0\n",
      "72832/95646 [=====================>........] - ETA: 7s - loss: 408.6470 - acc: 0.0000e+0\n",
      "74368/95646 [======================>.......] - ETA: 7s - loss: 406.4402 - acc: 0.0000e+0\n",
      "75520/95646 [======================>.......] - ETA: 6s - loss: 402.6860 - acc: 0.0000e+0\n",
      "76928/95646 [=======================>......] - ETA: 6s - loss: 400.0507 - acc: 0.0000e+0\n",
      "78464/95646 [=======================>......] - ETA: 5s - loss: 400.8353 - acc: 0.0000e+0\n",
      "79744/95646 [========================>.....] - ETA: 5s - loss: 398.2428 - acc: 0.0000e+0\n",
      "81152/95646 [========================>.....] - ETA: 4s - loss: 408.6503 - acc: 0.0000e+0\n",
      "82560/95646 [========================>.....] - ETA: 4s - loss: 406.0722 - acc: 0.0000e+0\n",
      "84096/95646 [=========================>....] - ETA: 3s - loss: 402.6818 - acc: 0.0000e+0\n",
      "84992/95646 [=========================>....] - ETA: 3s - loss: 399.4628 - acc: 0.0000e+0\n",
      "86400/95646 [==========================>...] - ETA: 3s - loss: 397.3237 - acc: 0.0000e+0\n",
      "87808/95646 [==========================>...] - ETA: 2s - loss: 396.3879 - acc: 0.0000e+0\n",
      "89088/95646 [==========================>...] - ETA: 2s - loss: 396.9067 - acc: 0.0000e+0\n",
      "90496/95646 [===========================>..] - ETA: 1s - loss: 393.8519 - acc: 0.0000e+0\n",
      "92032/95646 [===========================>..] - ETA: 1s - loss: 389.5822 - acc: 0.0000e+0\n",
      "93568/95646 [============================>.] - ETA: 0s - loss: 389.0793 - acc: 0.0000e+0\n",
      "94848/95646 [============================>.] - ETA: 0s - loss: 388.0371 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 31s 326us/sample - loss: 390.6632 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      " 1280/95646 [..............................] - ETA: 29s - loss: 139.0159 - acc: 0.0000e+\n",
      " 2560/95646 [..............................] - ETA: 29s - loss: 150.2920 - acc: 0.0000e+\n",
      " 3840/95646 [>.............................] - ETA: 28s - loss: 143.3357 - acc: 0.0000e+0\n",
      " 5248/95646 [>.............................] - ETA: 26s - loss: 156.5837 - acc: 0.0000e+0\n",
      " 6656/95646 [=>............................] - ETA: 26s - loss: 162.7804 - acc: 0.0000e+\n",
      " 7680/95646 [=>............................] - ETA: 26s - loss: 171.0351 - acc: 0.0000e+\n",
      " 9216/95646 [=>............................] - ETA: 26s - loss: 164.9967 - acc: 0.0000e+\n",
      "10752/95646 [==>...........................] - ETA: 25s - loss: 166.4117 - acc: 0.0000e+\n",
      "12032/95646 [==>...........................] - ETA: 24s - loss: 193.3091 - acc: 0.0000e+0\n",
      "13184/95646 [===>..........................] - ETA: 25s - loss: 192.3166 - acc: 0.0000e+0\n",
      "14592/95646 [===>..........................] - ETA: 24s - loss: 190.2586 - acc: 0.0000e+\n",
      "15872/95646 [===>..........................] - ETA: 24s - loss: 194.5741 - acc: 0.0000e+\n",
      "17408/95646 [====>.........................] - ETA: 23s - loss: 190.2642 - acc: 0.0000e+\n",
      "18816/95646 [====>.........................] - ETA: 23s - loss: 187.3187 - acc: 0.0000e+\n",
      "19712/95646 [=====>........................] - ETA: 23s - loss: 188.6107 - acc: 0.0000e+0\n",
      "20736/95646 [=====>........................] - ETA: 23s - loss: 210.1517 - acc: 0.0000e+0\n",
      "22144/95646 [=====>........................] - ETA: 23s - loss: 208.8053 - acc: 0.0000e+\n",
      "23424/95646 [======>.......................] - ETA: 22s - loss: 211.6625 - acc: 0.0000e+\n",
      "24960/95646 [======>.......................] - ETA: 21s - loss: 210.0977 - acc: 0.0000e+\n",
      "26368/95646 [=======>......................] - ETA: 21s - loss: 208.6750 - acc: 0.0000e+\n",
      "27648/95646 [=======>......................] - ETA: 21s - loss: 234.3353 - acc: 0.0000e+0\n",
      "29056/95646 [========>.....................] - ETA: 20s - loss: 232.2806 - acc: 0.0000e+\n",
      "30592/95646 [========>.....................] - ETA: 20s - loss: 231.4339 - acc: 0.0000e+\n",
      "31744/95646 [========>.....................] - ETA: 19s - loss: 229.0859 - acc: 0.0000e+\n",
      "32768/95646 [=========>....................] - ETA: 19s - loss: 234.1693 - acc: 0.0000e+\n",
      "33920/95646 [=========>....................] - ETA: 19s - loss: 231.8926 - acc: 0.0000e+\n",
      "35200/95646 [==========>...................] - ETA: 18s - loss: 232.2185 - acc: 0.0000e+0\n",
      "36736/95646 [==========>...................] - ETA: 18s - loss: 231.0215 - acc: 0.0000e+\n",
      "38272/95646 [==========>...................] - ETA: 17s - loss: 229.9222 - acc: 0.0000e+\n",
      "39552/95646 [===========>..................] - ETA: 17s - loss: 229.4138 - acc: 0.0000e+\n",
      "40832/95646 [===========>..................] - ETA: 16s - loss: 232.2336 - acc: 0.0000e+\n",
      "42368/95646 [============>.................] - ETA: 16s - loss: 233.3078 - acc: 0.0000e+\n",
      "43648/95646 [============>.................] - ETA: 15s - loss: 231.2934 - acc: 0.0000e+0\n",
      "45184/95646 [=============>................] - ETA: 15s - loss: 228.6798 - acc: 0.0000e+\n",
      "46336/95646 [=============>................] - ETA: 15s - loss: 227.0511 - acc: 0.0000e+\n",
      "47616/95646 [=============>................] - ETA: 14s - loss: 227.4979 - acc: 0.0000e+\n",
      "49152/95646 [==============>...............] - ETA: 14s - loss: 227.5010 - acc: 0.0000e+\n",
      "50432/95646 [==============>...............] - ETA: 13s - loss: 231.2165 - acc: 0.0000e+0\n",
      "51968/95646 [===============>..............] - ETA: 13s - loss: 231.9004 - acc: 0.0000e+0\n",
      "53376/95646 [===============>..............] - ETA: 12s - loss: 231.2298 - acc: 0.0000e+\n",
      "54272/95646 [================>.............] - ETA: 12s - loss: 232.1054 - acc: 0.0000e+\n",
      "55424/95646 [================>.............] - ETA: 12s - loss: 230.4057 - acc: 0.0000e+\n",
      "56832/95646 [================>.............] - ETA: 11s - loss: 229.1970 - acc: 0.0000e+\n",
      "58112/95646 [=================>............] - ETA: 11s - loss: 230.0026 - acc: 0.0000e+0\n",
      "59392/95646 [=================>............] - ETA: 11s - loss: 235.7794 - acc: 0.0000e+0\n",
      "60800/95646 [==================>...........] - ETA: 10s - loss: 234.5758 - acc: 0.0000e+\n",
      "61824/95646 [==================>...........] - ETA: 10s - loss: 233.2242 - acc: 0.0000e+\n",
      "63360/95646 [==================>...........] - ETA: 9s - loss: 231.3649 - acc: 0.0000e+00\n",
      "64896/95646 [===================>..........] - ETA: 9s - loss: 230.7937 - acc: 0.0000e+0\n",
      "66048/95646 [===================>..........] - ETA: 9s - loss: 229.2402 - acc: 0.0000e+0\n",
      "67328/95646 [====================>.........] - ETA: 8s - loss: 228.7543 - acc: 0.0000e+0\n",
      "68864/95646 [====================>.........] - ETA: 8s - loss: 280.1274 - acc: 0.0000e+0\n",
      "70400/95646 [=====================>........] - ETA: 7s - loss: 277.3137 - acc: 0.0000e+0\n",
      "71680/95646 [=====================>........] - ETA: 7s - loss: 275.6100 - acc: 0.0000e+0\n",
      "72960/95646 [=====================>........] - ETA: 6s - loss: 273.1422 - acc: 0.0000e+0\n",
      "74496/95646 [======================>.......] - ETA: 6s - loss: 280.3241 - acc: 0.0000e+0\n",
      "76032/95646 [======================>.......] - ETA: 6s - loss: 278.4388 - acc: 0.0000e+0\n",
      "77184/95646 [=======================>......] - ETA: 5s - loss: 277.0629 - acc: 0.0000e+0\n",
      "78720/95646 [=======================>......] - ETA: 5s - loss: 275.1250 - acc: 0.0000e+0\n",
      "80128/95646 [========================>.....] - ETA: 4s - loss: 274.4353 - acc: 0.0000e+0\n",
      "81280/95646 [========================>.....] - ETA: 4s - loss: 279.2950 - acc: 0.0000e+0\n",
      "82816/95646 [========================>.....] - ETA: 3s - loss: 278.3816 - acc: 0.0000e+0\n",
      "84352/95646 [=========================>....] - ETA: 3s - loss: 276.5948 - acc: 0.0000e+0\n",
      "85760/95646 [=========================>....] - ETA: 3s - loss: 276.7098 - acc: 0.0000e+0\n",
      "86656/95646 [==========================>...] - ETA: 2s - loss: 274.8617 - acc: 0.0000e+0\n",
      "88192/95646 [==========================>...] - ETA: 2s - loss: 274.3346 - acc: 0.0000e+0\n",
      "89728/95646 [===========================>..] - ETA: 1s - loss: 307.5312 - acc: 0.0000e+0\n",
      "91008/95646 [===========================>..] - ETA: 1s - loss: 307.3017 - acc: 0.0000e+0\n",
      "92416/95646 [===========================>..] - ETA: 0s - loss: 305.5549 - acc: 0.0000e+0\n",
      "93824/95646 [============================>.] - ETA: 0s - loss: 303.8283 - acc: 0.0000e+0\n",
      "95360/95646 [============================>.] - ETA: 0s - loss: 302.4690 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 29s 305us/sample - loss: 302.4583 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      " 1408/95646 [..............................] - ETA: 26s - loss: 276.5284 - acc: 0.0000e+00\n",
      " 2944/95646 [..............................] - ETA: 26s - loss: 203.2766 - acc: 0.0000e+\n",
      " 3840/95646 [>.............................] - ETA: 28s - loss: 187.1308 - acc: 0.0000e+\n",
      " 5376/95646 [>.............................] - ETA: 27s - loss: 165.4645 - acc: 0.0000e+\n",
      " 6912/95646 [=>............................] - ETA: 26s - loss: 152.7663 - acc: 0.0000e+\n",
      " 8192/95646 [=>............................] - ETA: 25s - loss: 155.5055 - acc: 0.0000e+0\n",
      " 9728/95646 [==>...........................] - ETA: 25s - loss: 153.5121 - acc: 0.0000e+0\n",
      "11136/95646 [==>...........................] - ETA: 25s - loss: 148.9432 - acc: 0.0000e+\n",
      "12288/95646 [==>...........................] - ETA: 24s - loss: 159.1084 - acc: 0.0000e+\n",
      "13824/95646 [===>..........................] - ETA: 24s - loss: 158.4260 - acc: 0.0000e+\n",
      "15360/95646 [===>..........................] - ETA: 23s - loss: 162.7847 - acc: 0.0000e+\n",
      "16384/95646 [====>.........................] - ETA: 24s - loss: 163.8326 - acc: 0.0000e+0\n",
      "17664/95646 [====>.........................] - ETA: 23s - loss: 159.6241 - acc: 0.0000e+0\n",
      "19200/95646 [=====>........................] - ETA: 23s - loss: 159.6141 - acc: 0.0000e+\n",
      "20480/95646 [=====>........................] - ETA: 22s - loss: 313.7776 - acc: 0.0000e+\n",
      "21888/95646 [=====>........................] - ETA: 22s - loss: 300.2237 - acc: 0.0000e+00\n",
      "23424/95646 [======>.......................] - ETA: 21s - loss: 298.7234 - acc: 0.0000e+\n",
      "24448/95646 [======>.......................] - ETA: 21s - loss: 291.2857 - acc: 0.0000e+0\n",
      "25856/95646 [=======>......................] - ETA: 21s - loss: 293.3501 - acc: 0.0000e+\n",
      "27392/95646 [=======>......................] - ETA: 20s - loss: 287.2556 - acc: 0.0000e+\n",
      "28672/95646 [=======>......................] - ETA: 20s - loss: 289.4943 - acc: 0.0000e+\n",
      "29824/95646 [========>.....................] - ETA: 20s - loss: 285.9631 - acc: 0.0000e+\n",
      "31360/95646 [========>.....................] - ETA: 19s - loss: 283.1811 - acc: 0.0000e+\n",
      "32640/95646 [=========>....................] - ETA: 19s - loss: 276.8195 - acc: 0.0000e+0\n",
      "34048/95646 [=========>....................] - ETA: 18s - loss: 271.4286 - acc: 0.0000e+\n",
      "35584/95646 [==========>...................] - ETA: 18s - loss: 266.6977 - acc: 0.0000e+\n",
      "36608/95646 [==========>...................] - ETA: 18s - loss: 263.8707 - acc: 0.0000e+\n",
      "38016/95646 [==========>...................] - ETA: 17s - loss: 262.4820 - acc: 0.0000e+\n",
      "39552/95646 [===========>..................] - ETA: 17s - loss: 266.4941 - acc: 0.0000e+\n",
      "40832/95646 [===========>..................] - ETA: 16s - loss: 262.3328 - acc: 0.0000e+0\n",
      "42240/95646 [============>.................] - ETA: 16s - loss: 258.0168 - acc: 0.0000e+\n",
      "43692/95646 [============>.................] - ETA: 15s - loss: 254.7447 - acc: 0.0000e+\n",
      "44800/95646 [=============>................] - ETA: 15s - loss: 265.9107 - acc: 0.0000e+\n",
      "46080/95646 [=============>................] - ETA: 15s - loss: 262.5380 - acc: 0.0000e+\n",
      "47360/95646 [=============>................] - ETA: 14s - loss: 258.4582 - acc: 0.0000e+0\n",
      "48896/95646 [==============>...............] - ETA: 14s - loss: 261.7399 - acc: 0.0000e+0\n",
      "50304/95646 [==============>...............] - ETA: 13s - loss: 258.5186 - acc: 0.0000e+\n",
      "51328/95646 [===============>..............] - ETA: 13s - loss: 256.0255 - acc: 0.0000e+\n",
      "52736/95646 [===============>..............] - ETA: 13s - loss: 254.6345 - acc: 0.0000e+\n",
      "54272/95646 [================>.............] - ETA: 12s - loss: 252.1820 - acc: 0.0000e+\n",
      "55296/95646 [================>.............] - ETA: 12s - loss: 249.1935 - acc: 0.0000e+0\n",
      "56320/95646 [================>.............] - ETA: 12s - loss: 247.0965 - acc: 0.0000e+0\n",
      "57600/95646 [=================>............] - ETA: 11s - loss: 245.2130 - acc: 0.0000e+\n",
      "58880/95646 [=================>............] - ETA: 11s - loss: 242.3278 - acc: 0.0000e+\n",
      "60416/95646 [=================>............] - ETA: 10s - loss: 240.1753 - acc: 0.0000e+\n",
      "61952/95646 [==================>...........] - ETA: 10s - loss: 239.1658 - acc: 0.0000e+\n",
      "63104/95646 [==================>...........] - ETA: 9s - loss: 237.6782 - acc: 0.0000e+00\n",
      "64384/95646 [===================>..........] - ETA: 9s - loss: 235.2631 - acc: 0.0000e+0\n",
      "65920/95646 [===================>..........] - ETA: 9s - loss: 232.9305 - acc: 0.0000e+0\n",
      "67456/95646 [====================>.........] - ETA: 8s - loss: 230.1593 - acc: 0.0000e+0\n",
      "68480/95646 [====================>.........] - ETA: 8s - loss: 229.6902 - acc: 0.0000e+0\n",
      "69888/95646 [====================>.........] - ETA: 7s - loss: 228.0034 - acc: 0.0000e+0\n",
      "71424/95646 [=====================>........] - ETA: 7s - loss: 226.5003 - acc: 0.0000e+0\n",
      "72704/95646 [=====================>........] - ETA: 7s - loss: 224.0269 - acc: 0.0000e+0\n",
      "74240/95646 [======================>.......] - ETA: 6s - loss: 257.7683 - acc: 0.0000e+0\n",
      "75520/95646 [======================>.......] - ETA: 6s - loss: 258.0794 - acc: 0.0000e+0\n",
      "76928/95646 [=======================>......] - ETA: 5s - loss: 256.9459 - acc: 0.0000e+0\n",
      "78208/95646 [=======================>......] - ETA: 5s - loss: 254.7518 - acc: 0.0000e+0\n",
      "79744/95646 [========================>.....] - ETA: 4s - loss: 252.1005 - acc: 0.0000e+0\n",
      "81280/95646 [========================>.....] - ETA: 4s - loss: 249.9783 - acc: 0.0000e+0\n",
      "82176/95646 [========================>.....] - ETA: 4s - loss: 249.1743 - acc: 0.0000e+0\n",
      "83712/95646 [=========================>....] - ETA: 3s - loss: 246.8660 - acc: 0.0000e+0\n",
      "85248/95646 [=========================>....] - ETA: 3s - loss: 244.3536 - acc: 0.0000e+0\n",
      "86656/95646 [==========================>...] - ETA: 2s - loss: 244.1934 - acc: 0.0000e+0\n",
      "87552/95646 [==========================>...] - ETA: 2s - loss: 243.2617 - acc: 0.0000e+0\n",
      "88320/95646 [==========================>...] - ETA: 2s - loss: 242.3485 - acc: 0.0000e+0\n",
      "89088/95646 [==========================>...] - ETA: 2s - loss: 241.4276 - acc: 0.0000e+0\n",
      "90284/95646 [===========================>..] - ETA: 1s - loss: 240.0385 - acc: 0.0000e+0\n",
      "91392/95646 [===========================>..] - ETA: 1s - loss: 238.1861 - acc: 0.0000e+0\n",
      "92288/95646 [===========================>..] - ETA: 1s - loss: 237.0676 - acc: 0.0000e+0\n",
      "93312/95646 [============================>.] - ETA: 0s - loss: 236.1961 - acc: 0.0000e+0\n",
      "94592/95646 [============================>.] - ETA: 0s - loss: 234.4255 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 318us/sample - loss: 233.0926 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      " 1408/95646 [..............................] - ETA: 31s - loss: 82.3171 - acc: 0.0000e+00\n",
      " 2176/95646 [..............................] - ETA: 37s - loss: 99..5489 - acc: 0.0000e+\n",
      " 3200/95646 [..............................] - ETA: 39s - loss: 111.4229 - acc: 0.0000e+\n",
      " 4352/95646 [>.............................] - ETA: 36s - loss: 120.4499 - acc: 0.0000e+0\n",
      " 5632/95646 [>.............................] - ETA: 34s - loss: 114.5842 - acc: 0.0000e+\n",
      " 6912/95646 [=>............................] - ETA: 33s - loss: 123.7497 - acc: 0.0000e+\n",
      " 8320/95646 [=>............................] - ETA: 31s - loss: 128.9143 - acc: 0.0000e+\n",
      " 9856/95646 [==>...........................] - ETA: 29s - loss: 127.9659 - acc: 0.0000e+\n",
      "11136/95646 [==>...........................] - ETA: 28s - loss: 121.3836 - acc: 0.0000e+0\n",
      "12544/95646 [==>...........................] - ETA: 27s - loss: 128.5744 - acc: 0.0000e+0\n",
      "13696/95646 [===>..........................] - ETA: 27s - loss: 125.3024 - acc: 0.0000e+\n",
      "14848/95646 [===>..........................] - ETA: 27s - loss: 267.8927 - acc: 0.0000e+\n",
      "16256/95646 [====>.........................] - ETA: 26s - loss: 253.2829 - acc: 0.0000e+\n",
      "17664/95646 [====>.........................] - ETA: 25s - loss: 242.2111 - acc: 0.0000e+\n",
      "18944/95646 [====>.........................] - ETA: 25s - loss: 245.3670 - acc: 0.0000e+0\n",
      "20096/95646 [=====>........................] - ETA: 25s - loss: 236.9387 - acc: 0.0000e+0\n",
      "21504/95646 [=====>........................] - ETA: 24s - loss: 230.0936 - acc: 0.0000e+\n",
      "22528/95646 [======>.......................] - ETA: 24s - loss: 221.6464 - acc: 0.0000e+\n",
      "24064/95646 [======>.......................] - ETA: 23s - loss: 215.3905 - acc: 0.0000e+\n",
      "25600/95646 [=======>......................] - ETA: 22s - loss: 210.9795 - acc: 0.0000e+\n",
      "26880/95646 [=======>......................] - ETA: 22s - loss: 204.5485 - acc: 0.0000e+0\n",
      "28032/95646 [=======>......................] - ETA: 22s - loss: 200.3077 - acc: 0.0000e+0\n",
      "29568/95646 [========>.....................] - ETA: 21s - loss: 212.4100 - acc: 0.0000e+\n",
      "30720/95646 [========>.....................] - ETA: 20s - loss: 208.4511 - acc: 0.0000e+\n",
      "32256/95646 [=========>....................] - ETA: 20s - loss: 203.8802 - acc: 0.0000e+00\n",
      "33408/95646 [=========>....................] - ETA: 20s - loss: 201.0187 - acc: 0.0000e+\n",
      "34688/95646 [=========>....................] - ETA: 19s - loss: 196.8229 - acc: 0.0000e+0\n",
      "36224/95646 [==========>...................] - ETA: 19s - loss: 192.9159 - acc: 0.0000e+\n",
      "37632/95646 [==========>...................] - ETA: 18s - loss: 191.3126 - acc: 0.0000e+\n",
      "38912/95646 [===========>..................] - ETA: 18s - loss: 186.7996 - acc: 0.0000e+\n",
      "40448/95646 [===========>..................] - ETA: 17s - loss: 182.6302 - acc: 0.0000e+\n",
      "41856/95646 [============>.................] - ETA: 17s - loss: 180.7277 - acc: 0.0000e+\n",
      "43008/95646 [============>.................] - ETA: 16s - loss: 177.4146 - acc: 0.0000e+0\n",
      "44544/95646 [============>.................] - ETA: 16s - loss: 176.3477 - acc: 0.0000e+\n",
      "46080/95646 [=============>................] - ETA: 15s - loss: 174.1512 - acc: 0.0000e+\n",
      "47232/95646 [=============>................] - ETA: 15s - loss: 172.1925 - acc: 0.0000e+\n",
      "48768/95646 [==============>...............] - ETA: 14s - loss: 173.4225 - acc: 0.0000e+\n",
      "50304/95646 [==============>...............] - ETA: 14s - loss: 171.2069 - acc: 0.0000e+\n",
      "51584/95646 [===============>..............] - ETA: 13s - loss: 168.6670 - acc: 0.0000e+0\n",
      "52864/95646 [===============>..............] - ETA: 13s - loss: 166.9349 - acc: 0.0000e+\n",
      "53860/95646 [===============>..............] - ETA: 13s - loss: 165.7939 - acc: 0.0000e+\n",
      "55040/95646 [================>.............] - ETA: 12s - loss: 163.8312 - acc: 0.0000e+\n",
      "56576/95646 [================>.............] - ETA: 12s - loss: 202.7268 - acc: 0.0000e+\n",
      "57856/95646 [=================>............] - ETA: 11s - loss: 201.1734 - acc: 0.0000e+0\n",
      "59264/95646 [=================>............] - ETA: 11s - loss: 207.7021 - acc: 0.0000e+0\n",
      "60672/95646 [==================>...........] - ETA: 11s - loss: 205.1921 - acc: 0.0000e+\n",
      "61568/95646 [==================>...........] - ETA: 10s - loss: 207.7125 - acc: 0.0000e+\n",
      "62976/95646 [==================>...........] - ETA: 10s - loss: 206.0854 - acc: 0.0000e+\n",
      "64512/95646 [===================>..........] - ETA: 9s - loss: 203.9404 - acc: 0.0000e+00\n",
      "65736/95646 [===================>..........] - ETA: 9s - loss: 202.1893 - acc: 0.0000e+0\n",
      "66688/95646 [===================>..........] - ETA: 9s - loss: 201.4641 - acc: 0.0000e+0\n",
      "68096/95646 [====================>.........] - ETA: 8s - loss: 204.4236 - acc: 0.0000e+0\n",
      "69632/95646 [====================>.........] - ETA: 8s - loss: 202.5440 - acc: 0.0000e+0\n",
      "70912/95646 [=====================>........] - ETA: 7s - loss: 200.8032 - acc: 0.0000e+0\n",
      "72192/95646 [=====================>........] - ETA: 7s - loss: 198.6579 - acc: 0.0000e+0\n",
      "73600/95646 [======================>.......] - ETA: 6s - loss: 197.0307 - acc: 0.0000e+0\n",
      "75008/95646 [======================>.......] - ETA: 6s - loss: 194.9845 - acc: 0.0000e+0\n",
      "76160/95646 [======================>.......] - ETA: 6s - loss: 193.4484 - acc: 0.0000e+0\n",
      "77696/95646 [=======================>......] - ETA: 5s - loss: 191.8392 - acc: 0.0000e+0\n",
      "79104/95646 [=======================>......] - ETA: 5s - loss: 190.1945 - acc: 0.0000e+0\n",
      "80256/95646 [========================>.....] - ETA: 4s - loss: 188.3786 - acc: 0.0000e+0\n",
      "81792/95646 [========================>.....] - ETA: 4s - loss: 186.8265 - acc: 0.0000e+0\n",
      "83328/95646 [=========================>....] - ETA: 3s - loss: 184.8827 - acc: 0.0000e+0\n",
      "84608/95646 [=========================>....] - ETA: 3s - loss: 183.9225 - acc: 0.0000e+0\n",
      "85632/95646 [=========================>....] - ETA: 3s - loss: 182.4087 - acc: 0.0000e+0\n",
      "86912/95646 [==========================>...] - ETA: 2s - loss: 181.6442 - acc: 0.0000e+0\n",
      "88448/95646 [==========================>...] - ETA: 2s - loss: 181.0110 - acc: 0.0000e+0\n",
      "89728/95646 [===========================>..] - ETA: 1s - loss: 179.8147 - acc: 0.0000e+0\n",
      "91008/95646 [===========================>..] - ETA: 1s - loss: 178.6533 - acc: 0.0000e+0\n",
      "92160/95646 [===========================>..] - ETA: 1s - loss: 177.5578 - acc: 0.0000e+0\n",
      "93696/95646 [============================>.] - ETA: 0s - loss: 179.3757 - acc: 0.0000e+0\n",
      "94848/95646 [============================>.] - ETA: 0s - loss: 179.8644 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 314us/sample - loss: 179.3457 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      " 1408/95646 [..............................] - ETA: 27s - loss: 86.0227 - acc: 0.0000e+0\n",
      " 2816/95646 [..............................] - ETA: 29s - loss: 206.8852 - acc: 0.0000e+\n",
      " 4224/95646 [>.............................] - ETA: 29s - loss: 165.2578 - acc: 0.0000e+\n",
      " 5376/95646 [>.............................] - ETA: 27s - loss: 156.8033 - acc: 0.0000e+0\n",
      " 6912/95646 [=>............................] - ETA: 26s - loss: 145.7842 - acc: 0.0000e+\n",
      " 8192/95646 [=>............................] - ETA: 25s - loss: 129.3011 - acc: 0.0000e+\n",
      " 9600/95646 [==>...........................] - ETA: 25s - loss: 131.0358 - acc: 0.0000e+\n",
      "11008/95646 [==>...........................] - ETA: 25s - loss: 122.7188 - acc: 0.0000e+\n",
      "12032/95646 [==>...........................] - ETA: 25s - loss: 115.1574 - acc: 0.0000e+0\n",
      "13568/95646 [===>..........................] - ETA: 24s - loss: 109.1120 - acc: 0.0000e+0\n",
      "14848/95646 [===>..........................] - ETA: 24s - loss: 105.4674 - acc: 0.0000e+\n",
      "16128/95646 [====>.........................] - ETA: 24s - loss: 108.7738 - acc: 0.0000e+\n",
      "17536/95646 [====>.........................] - ETA: 23s - loss: 108.0172 - acc: 0.0000e+\n",
      "19072/95646 [====>.........................] - ETA: 23s - loss: 105.3172 - acc: 0.0000e+\n",
      "20096/95646 [=====>........................] - ETA: 23s - loss: 105.7519 - acc: 0.0000e+0\n",
      "21504/95646 [=====>........................] - ETA: 22s - loss: 111.9814 - acc: 0.0000e+0\n",
      "22528/95646 [======>.......................] - ETA: 22s - loss: 110.6606 - acc: 0.0000e+\n",
      "23424/95646 [======>.......................] - ETA: 22s - loss: 117.7669 - acc: 0.0000e+\n",
      "24960/95646 [======>.......................] - ETA: 22s - loss: 115.2658 - acc: 0.0000e+\n",
      "26496/95646 [=======>......................] - ETA: 21s - loss: 113.1090 - acc: 0.0000e+\n",
      "27520/95646 [=======>......................] - ETA: 21s - loss: 111.3613 - acc: 0.0000e+0\n",
      "28928/95646 [========>.....................] - ETA: 20s - loss: 108.6952 - acc: 0.0000e+0\n",
      "30208/95646 [========>.....................] - ETA: 20s - loss: 106.9382 - acc: 0.0000e+\n",
      "31488/95646 [========>.....................] - ETA: 20s - loss: 106.0175 - acc: 0.0000e+\n",
      "33024/95646 [=========>....................] - ETA: 19s - loss: 126.0584 - acc: 0.0000e+00\n",
      "34432/95646 [=========>....................] - ETA: 19s - loss: 124.2831 - acc: 0.0000e+\n",
      "35584/95646 [==========>...................] - ETA: 18s - loss: 121.6966 - acc: 0.0000e+0\n",
      "37120/95646 [==========>...................] - ETA: 18s - loss: 124.2761 - acc: 0.0000e+\n",
      "38656/95646 [===========>..................] - ETA: 17s - loss: 122.8675 - acc: 0.0000e+\n",
      "39808/95646 [===========>..................] - ETA: 17s - loss: 122.6448 - acc: 0.0000e+\n",
      "40960/95646 [===========>..................] - ETA: 17s - loss: 120.6584 - acc: 0.0000e+\n",
      "42496/95646 [============>.................] - ETA: 16s - loss: 119.8308 - acc: 0.0000e+\n",
      "43648/95646 [============>.................] - ETA: 16s - loss: 117.9299 - acc: 0.0000e+0\n",
      "44928/95646 [=============>................] - ETA: 15s - loss: 117.2594 - acc: 0.0000e+\n",
      "46464/95646 [=============>................] - ETA: 15s - loss: 116.4467 - acc: 0.0000e+\n",
      "47616/95646 [=============>................] - ETA: 14s - loss: 114.7869 - acc: 0.0000e+\n",
      "48768/95646 [==============>...............] - ETA: 14s - loss: 114.6512 - acc: 0.0000e+\n",
      "50304/95646 [==============>...............] - ETA: 14s - loss: 113.6249 - acc: 0.0000e+\n",
      "51584/95646 [===============>..............] - ETA: 13s - loss: 149.3833 - acc: 0.0000e+0\n",
      "52992/95646 [===============>..............] - ETA: 13s - loss: 147.6267 - acc: 0.0000e+\n",
      "54244/95646 [===============>..............] - ETA: 12s - loss: 145.6161 - acc: 0.0000e+\n",
      "55552/95646 [================>.............] - ETA: 12s - loss: 146.0288 - acc: 0.0000e+\n",
      "57088/95646 [================>.............] - ETA: 11s - loss: 144.6500 - acc: 0.0000e+\n",
      "58240/95646 [=================>............] - ETA: 11s - loss: 143.6437 - acc: 0.0000e+0\n",
      "59776/95646 [=================>............] - ETA: 11s - loss: 143.0067 - acc: 0.0000e+0\n",
      "61056/95646 [==================>...........] - ETA: 10s - loss: 141.9252 - acc: 0.0000e+\n",
      "61696/95646 [==================>...........] - ETA: 10s - loss: 140.8375 - acc: 0.0000e+\n",
      "63104/95646 [==================>...........] - ETA: 10s - loss: 139.4872 - acc: 0.0000e+\n",
      "64640/95646 [===================>..........] - ETA: 9s - loss: 138.2366 - acc: 0.0000e+00\n",
      "66176/95646 [===================>..........] - ETA: 9s - loss: 136.2744 - acc: 0.0000e+0\n",
      "67200/95646 [====================>.........] - ETA: 8s - loss: 136.2949 - acc: 0.0000e+0\n",
      "68608/95646 [====================>.........] - ETA: 8s - loss: 135.3465 - acc: 0.0000e+0\n",
      "70144/95646 [=====================>........] - ETA: 7s - loss: 135.3178 - acc: 0.0000e+0\n",
      "71424/95646 [=====================>........] - ETA: 7s - loss: 134.0484 - acc: 0.0000e+0\n",
      "72960/95646 [=====================>........] - ETA: 7s - loss: 133.1787 - acc: 0.0000e+0\n",
      "74112/95646 [======================>.......] - ETA: 6s - loss: 132.2470 - acc: 0.0000e+0\n",
      "75520/95646 [======================>.......] - ETA: 6s - loss: 131.0754 - acc: 0.0000e+0\n",
      "76672/95646 [=======================>......] - ETA: 5s - loss: 130.1410 - acc: 0.0000e+0\n",
      "78208/95646 [=======================>......] - ETA: 5s - loss: 128.7167 - acc: 0.0000e+0\n",
      "79616/95646 [=======================>......] - ETA: 5s - loss: 127.9562 - acc: 0.0000e+0\n",
      "80896/95646 [========================>.....] - ETA: 4s - loss: 150.8384 - acc: 0.0000e+0\n",
      "82432/95646 [========================>.....] - ETA: 4s - loss: 149.7429 - acc: 0.0000e+0\n",
      "83840/95646 [=========================>....] - ETA: 3s - loss: 148.3483 - acc: 0.0000e+0\n",
      "85376/95646 [=========================>....] - ETA: 3s - loss: 147.2151 - acc: 0.0000e+0\n",
      "86400/95646 [==========================>...] - ETA: 2s - loss: 145.9281 - acc: 0.0000e+0\n",
      "87552/95646 [==========================>...] - ETA: 2s - loss: 144.8705 - acc: 0.0000e+0\n",
      "88960/95646 [==========================>...] - ETA: 2s - loss: 143.9945 - acc: 0.0000e+0\n",
      "90112/95646 [===========================>..] - ETA: 1s - loss: 142.9812 - acc: 0.0000e+0\n",
      "91648/95646 [===========================>..] - ETA: 1s - loss: 142.1958 - acc: 0.0000e+0\n",
      "93184/95646 [============================>.] - ETA: 0s - loss: 141.1079 - acc: 0.0000e+0\n",
      "94720/95646 [============================>.] - ETA: 0s - loss: 140.0577 - acc: 0.0000e+0\n",
      "95646/95646 [==============================] - 30s 309us/sample - loss: 139.3791 - acc: 0.0000e+00\n",
      "Model export success: %s trained_wine_model.dat\n",
      "eval model 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/function_shim.py\", line 79, in <module>\n",
      "    call(args.serialized_fn_file)\n",
      "  File \"/app/function_shim.py\", line 66, in call\n",
      "    res = obj().train()\n",
      "  File \"<ipython-input-4-b8a15908cae1>\", line 196, in train\n",
      "  File \"<ipython-input-4-b8a15908cae1>\", line 122, in eval_model\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 143, in load_model\n",
      "    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 160, in load_model_from_hdf5\n",
      "    model_config = json.loads(model_config.decode('utf-8'))\n",
      "AttributeError: 'str' object has no attribute 'decode'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning up job fairing-job-9bs5k...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairing-job-9bs5k'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kubeflow.fairing import TrainJob\n",
    "train_job = TrainJob(WinePricer, input_files=['wine_data.csv', \"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY,\n",
    "                     backend=BackendClass(build_context_source=BuildContext))\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.fairing import PredictionEndpoint\n",
    "endpoint = PredictionEndpoint(WinePricer, input_files=['trained_wine_model.dat', \"requirements.txt\"],\n",
    "                              docker_registry=DOCKER_REGISTRY,\n",
    "                              service_type='ClusterIP',\n",
    "                              backend=BackendClass(build_context_source=BuildContext))\n",
    "endpoint.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait service a while to be ready and replace `<endpoint>` with the output from last step.\n",
    "# Here's an example !nc -vz fairing-service-srwh2.anonymous.svc.cluster.local 5000\n",
    "\n",
    "!netcat fairing-service-rfmnj.eksworkspace.svc.cluster.local 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PR https://github.com/kubeflow/fairing/pull/376\n",
    "# Add `:5000/predict` to mitigate the issue.\n",
    "endpoint.url='http://fairing-service-n8qv2.anonymous.svc.cluster.local:5000/predict'\n",
    "\n",
    "endpoint.predict_nparray(test_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
